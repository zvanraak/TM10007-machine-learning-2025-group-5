{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples: 115\n",
      "The number of columns: 494\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "#from worcgist.load_data import load_data\n",
    "from worclipo.load_data import load_data\n",
    "#from worcliver.load_data import load_data\n",
    "#from ecg.load_data import load_data\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn import model_selection\n",
    "\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')\n",
    "\n",
    "X = data.drop(\"label\",axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data\n",
    "* Part 1: Finding missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Define missing value indicators\n",
    "custom_missing = ['NA', 'N/A', '?', 'None', 'none', '-']\n",
    "\n",
    "# Count NaNs\n",
    "nan_counts = X_train.isna().sum()\n",
    "\n",
    "# Count empty strings\n",
    "empty_string_counts = (X_train == '').sum()\n",
    "\n",
    "# Count custom missing indicators (case-insensitive match)\n",
    "custom_missing_counts = X_train.apply(lambda col: col.astype(str).str.lower().isin([val.lower() for val in custom_missing]).sum())\n",
    "\n",
    "# Compute total missing count per column\n",
    "total_missing = nan_counts + empty_string_counts + custom_missing_counts\n",
    "\n",
    "# Filter out columns where total missing is zero\n",
    "total_missing_selected = total_missing[total_missing != 0]\n",
    "\n",
    "# Print total missing counts\n",
    "print(total_missing_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Part 2: Processing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Replacing missing values with NaN\n",
    "custom_missing = ['NA', 'N/A', '?', 'None', 'none', '-', '']\n",
    "X_train.replace(custom_missing, np.nan, inplace=True)\n",
    "X_test.replace(custom_missing, np.nan, inplace=True)\n",
    "\n",
    "# If 50% or more of the data within one feature is missing the feature is deleted\n",
    "limit = len(X_train.index)*50/100\n",
    "valid_columns = [col for col, count in total_missing.items() if count < limit]\n",
    "\n",
    "# Keep only the valid columns in both X_train and X_test\n",
    "X_train = X_train[valid_columns]\n",
    "X_test = X_test[valid_columns]\n",
    "\n",
    "# Imputate \n",
    "\n",
    "# Check if imputation is needed\n",
    "if X_train.isna().sum().sum() == 0:\n",
    "    pass\n",
    "else:\n",
    "    # Dictionary to store mean/median decision per column\n",
    "    imputation_strategies = {}\n",
    "\n",
    "    for col in X_train.select_dtypes(include=['number']).columns:  # Only numeric columns\n",
    "        col_data = X_train[col].dropna()  # Remove NaN values for testing\n",
    "\n",
    "        if len(col_data) > 3:  # Shapiro requires at least 3 non-null values\n",
    "            if col_data.nunique() == 1:  # Check if all values are the same\n",
    "                strategy = 'median'  # Default to median if no variability\n",
    "            else:\n",
    "                _, p = shapiro(col_data)\n",
    "                strategy = 'mean' if p > 0.05 else 'median'\n",
    "        else:\n",
    "            strategy = 'median'  # Default to median if too few values\n",
    "\n",
    "        imputation_strategies[col] = strategy\n",
    "\n",
    "    # Create imputers for mean and median\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Apply imputers for each feature\n",
    "    for col, strategy in imputation_strategies.items():\n",
    "        imputer = mean_imputer if strategy == 'mean' else median_imputer\n",
    "        X_train[col] = imputer.fit_transform(X_train[[col]])\n",
    "        X_test[col] = imputer.transform(X_test[[col]])  # Use the same imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler_robust = preprocessing.RobustScaler()\n",
    "\n",
    "scaled_robust_array_train = scaler_robust.fit_transform(X_train)\n",
    "scaled_robust_array_test = scaler_robust.transform(X_test)\n",
    "\n",
    "X_scaled_robust_train = pd.DataFrame(scaled_robust_array_train, columns=X_train.columns)\n",
    "X_scaled_robust_test = pd.DataFrame(scaled_robust_array_test, columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lloyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "Random forest, decision tree and bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Comparison ===\n",
      "\n",
      "Best Random Forest: RandomForestClassifier(bootstrap=False, max_depth=17, min_samples_split=13,\n",
      "                       n_estimators=59)\n",
      "Best Random Forest Parameters: {'bootstrap': False, 'max_depth': 17, 'min_samples_split': 13, 'n_estimators': 59}\n",
      "Best Random Forest Accuracy: 0.7125\n",
      "\n",
      "Best Bagging: BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=6,\n",
      "                                                   min_samples_split=9),\n",
      "                  n_estimators=156)\n",
      "Best Bagging Parameters: {'bootstrap': True, 'estimator__max_depth': 6, 'estimator__min_samples_split': 9, 'n_estimators': 156}\n",
      "Best Bagging Accuracy: 0.7694\n",
      "\n",
      "Best Decision Tree: DecisionTreeClassifier(max_depth=24, min_samples_split=3)\n",
      "Best Decision Tree Parameters: {'max_depth': 24, 'min_samples_split': 3}\n",
      "Best Decision Tree Accuracy: 0.7167\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Define models and parameter distributions\n",
    "models = {\n",
    "    \"Random Forest\": (RandomForestClassifier(), {\n",
    "        'n_estimators': randint(5, 200),\n",
    "        'max_depth': randint(3, 30),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'bootstrap': [True, False]\n",
    "    }),\n",
    "    \"Bagging\": (BaggingClassifier(DecisionTreeClassifier()), {\n",
    "        'n_estimators': randint(5, 200),\n",
    "        'estimator__max_depth': randint(3, 30),\n",
    "        'estimator__min_samples_split': randint(2, 20),\n",
    "        'bootstrap': [True, False]\n",
    "    }),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), {\n",
    "        'max_depth': randint(3, 30),\n",
    "        'min_samples_split': randint(2, 20)\n",
    "    })\n",
    "}\n",
    "\n",
    "# Perform Randomized Search and store results\n",
    "best_estimators = {}\n",
    "best_params = {}\n",
    "best_scores = {}\n",
    "\n",
    "for name, (model, param_dist) in models.items():\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring='accuracy',\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train_encoded)\n",
    "    \n",
    "    best_estimators[name] = search.best_estimator_\n",
    "    best_params[name] = search.best_params_\n",
    "    best_scores[name] = search.best_score_\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "for name in models.keys():\n",
    "    print(f\"\\nBest {name}: {best_estimators[name]}\")\n",
    "    print(f\"Best {name} Parameters: {best_params[name]}\")\n",
    "    print(f\"Best {name} Accuracy: {best_scores[name]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
