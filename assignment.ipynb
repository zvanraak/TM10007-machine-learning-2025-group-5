{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples: 115\n",
      "The number of columns: 494\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "#from worcgist.load_data import load_data\n",
    "from worclipo.load_data import load_data\n",
    "#from worcliver.load_data import load_data\n",
    "#from ecg.load_data import load_data\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn import model_selection\n",
    "\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')\n",
    "\n",
    "X = data.drop(\"label\",axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data\n",
    "* Part 1: Finding missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n",
      "PREDICT_original_sf_area_min_2.5D                                       2\n",
      "PREDICT_original_tf_LBP_quartile_range_R8_P24                           4\n",
      "PREDICT_original_tf_LBP_peak_R15_P36                                    1\n",
      "PREDICT_original_tf_LBP_peak_position_R15_P36                           1\n",
      "PREDICT_original_vf_Frangi_full_quartile_range_SR(1.0, 10.0)_SS2.0      2\n",
      "PREDICT_original_vf_Frangi_edge_quartile_range_SR(1.0, 10.0)_SS2.0      2\n",
      "PREDICT_original_vf_Frangi_inner_quartile_range_SR(1.0, 10.0)_SS2.0     5\n",
      "PREDICT_original_phasef_monogenic_peak_WL3_N5                           4\n",
      "PREDICT_original_phasef_monogenic_peak_position_WL3_N5                  4\n",
      "PREDICT_original_phasef_phasecong_quartile_range_WL3_N5                10\n",
      "PREDICT_original_phasef_phasesym_quartile_range_WL3_N5                  6\n",
      "dtype: int64\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Define missing value indicators\n",
    "custom_missing = ['NA', 'null', 'N/A', '?', 'None', 'none', '-']\n",
    "\n",
    "# Count NaNs\n",
    "nan_counts = X_train.isna().sum()\n",
    "nan_counts_selected = nan_counts[nan_counts !=0]\n",
    "\n",
    "# Count empty strings\n",
    "empty_string_counts = (X_train == '').sum()\n",
    "empty_string_counts_selected = empty_string_counts[empty_string_counts != 0]\n",
    "\n",
    "# Count zero's\n",
    "zero_counts = (X_train == 0).sum()\n",
    "zero_counts_selected = zero_counts[zero_counts != 0]\n",
    "\n",
    "# Count custom missing indicators (case-insensitive match)\n",
    "custom_missing_counts = X_train.apply(lambda col: col.astype(str).str.lower().isin([val.lower() for val in custom_missing]).sum())\n",
    "custom_missing_counts_selected = custom_missing_counts[custom_missing_counts !=0]\n",
    "\n",
    "# Printing\n",
    "print(nan_counts_selected)\n",
    "print(empty_string_counts_selected)\n",
    "print(zero_counts_selected)\n",
    "print(custom_missing_counts_selected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Part 2: Processing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data was only find in the form of zero's, therefore only zero_counts is further used.\n",
    "limit = len(data.index)*20/100 #If 20% or more of the data wihtin one feature is missing the feature is deleted\n",
    "valid_columns = [col for col, count in zero_counts.items() if count < limit]\n",
    "\n",
    "# Keep only the valid columns in both X_train and X_test\n",
    "X_train = X_train[valid_columns]\n",
    "X_test = X_test[valid_columns]\n",
    "\n",
    "# Imputate remaining zero's\n",
    "# Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Replace zeros with NaN\n",
    "X_train.replace(0, np.nan, inplace=True)\n",
    "X_test.replace(0, np.nan, inplace=True)\n",
    "\n",
    "imputation_strategies = {}  # Store mean/median decision per column\n",
    "\n",
    "for col in X_train.select_dtypes(include=['number']).columns:  # Only numeric columns\n",
    "    col_data = X_train[col].dropna()  # Remove NaN values for testing\n",
    "\n",
    "    if len(col_data) > 3:  # Shapiro requires at least 3 non-null values\n",
    "        if col_data.nunique() == 1:  # Check if all values are the same\n",
    "            strategy = 'median'  # Default to median if no variability\n",
    "        else:\n",
    "            _, p = shapiro(col_data)\n",
    "            strategy = 'mean' if p > 0.05 else 'median'\n",
    "    else:\n",
    "        strategy = 'median'  # Default to median if too few values\n",
    "\n",
    "    imputation_strategies[col] = strategy\n",
    "\n",
    "# Create imputers for mean and median\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "for col, strategy in imputation_strategies.items():\n",
    "    imputer = mean_imputer if strategy == 'mean' else median_imputer\n",
    "    X_train[[col]] = imputer.fit_transform(X_train[[col]])\n",
    "    X_test[[col]] = imputer.transform(X_test[[col]])  # Use the same imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lloyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
