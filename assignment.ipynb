{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn utilities\n",
    "from sklearn import datasets as ds\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    cross_val_predict,\n",
    "    cross_val_score,\n",
    "    RandomizedSearchCV,\n",
    "    GridSearchCV,\n",
    "    learning_curve\n",
    ")\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    f_classif,\n",
    "    mutual_info_classif,\n",
    "    VarianceThreshold,\n",
    "    RFECV,\n",
    "    SelectFromModel\n",
    ")\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Statistical testing\n",
    "from scipy.stats import shapiro, randint\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    SGDClassifier,\n",
    "    Lasso,\n",
    "    LassoCV\n",
    ")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Other utilities\n",
    "import copy\n",
    "from sklearn.base import clone\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# Custom module\n",
    "from worclipo.load_data import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')\n",
    "\n",
    "X = data.drop(\"label\",axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #X, y, test_size=0.2, random_state=42, stratify=y\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General functions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions we will use\n",
    "def colorplot(clf, ax, x, y, h=100, precomputer=None):\n",
    "    '''\n",
    "    Overlay the decision areas as colors in an axes.\n",
    "\n",
    "    Input:\n",
    "        clf: trained classifier\n",
    "        ax: axis to overlay color mesh on\n",
    "        x: feature on x-axis\n",
    "        y: feature on y-axis\n",
    "        h(optional): steps in the mesh\n",
    "    '''\n",
    "    # Create a meshgrid the size of the axis\n",
    "    xstep = (x.max() - x.min() ) / 20.0\n",
    "    ystep = (y.max() - y.min() ) / 20.0\n",
    "    x_min, x_max = x.min() - xstep, x.max() + xstep\n",
    "    y_min, y_max = y.min() - ystep, y.max() + ystep\n",
    "    h = max((x_max - x_min, y_max - y_min))/h\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    features = np.c_[xx.ravel(), yy.ravel()]\n",
    "    if precomputer is not None:\n",
    "        if type(precomputer) is RBFSampler:\n",
    "            features = precomputer.transform(features)\n",
    "        elif precomputer is rbf_kernel:\n",
    "            features = rbf_kernel(features, X)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(features)\n",
    "    else:\n",
    "        Z = clf.predict_proba(features)\n",
    "    if len(Z.shape) > 1:\n",
    "        Z = Z[:, 1]\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    cm = plt.cm.RdBu_r\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "    del xx, yy, x_min, x_max, y_min, y_max, Z, cm\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "\n",
    "    axes.set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores  = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "def plot_roc_curve(y_score, y_truth):\n",
    "    '''\n",
    "    Plot an ROC curve.\n",
    "    '''\n",
    "    # Only take scores for class = 1\n",
    "    y_score = y_score[:, 1]\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(y_truth, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data\n",
    "* Part 1: Finding missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_data(X_train):\n",
    "\n",
    "    # Define missing value indicators\n",
    "    custom_missing = ['NA', 'N/A', '?', 'None', 'none', '-']\n",
    "\n",
    "    # Count NaNs\n",
    "    nan_counts = X_train.isna().sum()\n",
    "\n",
    "    # Count empty strings\n",
    "    empty_string_counts = (X_train == '').sum()\n",
    "\n",
    "    # Count custom missing indicators (case-insensitive match)\n",
    "    custom_missing_counts = X_train.apply(lambda col: col.astype(str).str.lower().isin([val.lower() for val in custom_missing]).sum())\n",
    "\n",
    "    # Compute total missing count per column\n",
    "    total_missing = nan_counts + empty_string_counts + custom_missing_counts\n",
    "\n",
    "    # Filter out columns where total missing is zero\n",
    "    total_missing_selected = total_missing[total_missing != 0]\n",
    "\n",
    "    # Print total missing counts\n",
    "    print(total_missing_selected)\n",
    "\n",
    "    return total_missing\n",
    "\n",
    "#total_missing = find_missing_data(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Part 2: Processing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_missing_data(X_train, X_test, total_missing):\n",
    "\n",
    "    # Replacing missing values with NaN\n",
    "    custom_missing = ['NA', 'N/A', '?', 'None', 'none', '-', '']\n",
    "    X_train.replace(custom_missing, np.nan, inplace=True)\n",
    "    X_test.replace(custom_missing, np.nan, inplace=True)\n",
    "\n",
    "    # If 50% or more of the data within one feature is missing the feature is deleted\n",
    "    limit = len(X_train.index)*50/100\n",
    "    valid_columns = [col for col, count in total_missing.items() if count < limit]\n",
    "\n",
    "    # Keep only the valid columns in both X_train and X_test\n",
    "    X_train = X_train[valid_columns]\n",
    "    X_test = X_test[valid_columns]\n",
    "\n",
    "    # Imputate\n",
    "\n",
    "    # Check if imputation is needed\n",
    "    if X_train.isna().sum().sum() == 0:\n",
    "        pass\n",
    "    else:\n",
    "        # Dictionary to store mean/median decision per column\n",
    "        imputation_strategies = {}\n",
    "\n",
    "        for col in X_train.select_dtypes(include=['number']).columns:  # Only numeric columns\n",
    "            col_data = X_train[col].dropna()  # Remove NaN values for testing\n",
    "\n",
    "            if len(col_data) > 3:  # Shapiro requires at least 3 non-null values\n",
    "                if col_data.nunique() == 1:  # Check if all values are the same\n",
    "                    strategy = 'median'  # Default to median if no variability\n",
    "                else:\n",
    "                    _, p = shapiro(col_data)\n",
    "                    strategy = 'mean' if p > 0.05 else 'median'\n",
    "            else:\n",
    "                strategy = 'median'  # Default to median if too few values\n",
    "\n",
    "            imputation_strategies[col] = strategy\n",
    "\n",
    "        # Create imputers for mean and median\n",
    "        mean_imputer = SimpleImputer(strategy='mean')\n",
    "        median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "        # Apply imputers for each feature\n",
    "        for col, strategy in imputation_strategies.items():\n",
    "            imputer = mean_imputer if strategy == 'mean' else median_imputer\n",
    "            X_train[col] = imputer.fit_transform(X_train[[col]])\n",
    "            X_test[col] = imputer.transform(X_test[[col]])  # Use the same imputer\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "#X_train, X_test = process_missing_data(X_train, X_test, total_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(X_train, X_test):\n",
    "\n",
    "    scaler_robust = preprocessing.RobustScaler()\n",
    "\n",
    "    scaled_robust_array_train = scaler_robust.fit_transform(X_train)\n",
    "    scaled_robust_array_test = scaler_robust.transform(X_test)\n",
    "\n",
    "    X_scaled_robust_train = pd.DataFrame(scaled_robust_array_train, columns=X_train.columns)\n",
    "    X_scaled_robust_test = pd.DataFrame(scaled_robust_array_test, columns=X_test.columns)\n",
    "\n",
    "    return X_scaled_robust_train, X_scaled_robust_test\n",
    "\n",
    "#X_scaled_robust_train, X_scaled_robust_test = scaling(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step: variance based thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_variance_threshold(X_train, X_test, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Applies variance thresholding to remove low-variance features.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Scaled training feature set.\n",
    "        X_test (pd.DataFrame): Scaled test feature set.\n",
    "        threshold (float): Threshold for variance. Features with variance below this will be removed.\n",
    "\n",
    "    Returns:\n",
    "        X_train_filtered (pd.DataFrame): Filtered training set.\n",
    "        X_test_filtered (pd.DataFrame): Filtered test set.\n",
    "        selected_features (List[str]): Names of the retained features.\n",
    "    \"\"\"\n",
    "    selector_variance_threshold = VarianceThreshold(threshold=threshold)\n",
    "    X_train_filtered_variance_np = selector_variance_threshold.fit_transform(X_train)\n",
    "    selected_features = X_train.columns[selector_variance_threshold.get_support()]\n",
    "    \n",
    "    X_train_filtered = pd.DataFrame(X_train_filtered_variance_np, columns=selected_features, index=X_train.index)\n",
    "    X_test_filtered = pd.DataFrame(selector_variance_threshold.transform(X_test), columns=selected_features, index=X_test.index)\n",
    "\n",
    "    print(f\"[VarianceThreshold] Remaining features: {len(selected_features)}\")\n",
    "    return X_train_filtered, X_test_filtered, selected_features\n",
    "\n",
    "#X_filtered_train_variance_new, X_filtered_test_variance_new, variance_filtered_features_new = apply_variance_threshold(X_scaled_robust_train, X_scaled_robust_test)\n",
    "#print(X_filtered_train_variance_new.shape)\n",
    "#print(X_filtered_test_variance_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering with ANOVA: linear relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_anova(X_train, y_train, X_test, k=75, plot=True):\n",
    "    \"\"\"\n",
    "    Selects the top-k features using ANOVA F-test (for linear dependencies).\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features. \n",
    "        y_train (pd.Series or np.array): Training labels, numerically encoded\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        k (int): Number of top features to select.\n",
    "        plot (bool): If True, visualizes F-statistics and top 2 feature scatter plot.\n",
    "\n",
    "    Returns:\n",
    "        X_train_anova (pd.DataFrame): Training set with selected features.\n",
    "        X_test_anova (pd.DataFrame): Test set with selected features.\n",
    "        selected_features (List[str]): Names of the selected features.\n",
    "    \"\"\"\n",
    "    selector_anova = SelectKBest(f_classif, k=k)\n",
    "    X_train_filtered_anova_np = selector_anova.fit_transform(X_train, y_train)\n",
    "    selected_features = X_train.columns[selector_anova.get_support()]\n",
    "    \n",
    "    X_train_anova = pd.DataFrame(X_train_filtered_anova_np, columns=selected_features, index=X_train.index)\n",
    "    X_test_anova = pd.DataFrame(selector_anova.transform(X_test), columns=selected_features, index=X_test.index)\n",
    "\n",
    "    print(f\"[ANOVA] Selected top-{k} features.\")\n",
    "    return X_train_anova, X_test_anova, selected_features\n",
    "\n",
    "# Transform labels to numeric values, i.e. 0 or 1 \n",
    "#le = LabelEncoder()\n",
    "#y_train_numeric = le.fit_transform(y_train)\n",
    "#legend_labels = le.classes_  # shows mapping from labels to numbers (for plotting/checking)\n",
    "\n",
    "#X_filtered_train_anova_new, X_filtered_test_anova_new, anova_selected_features_new = select_features_anova(X_filtered_train_variance_new, y_train_numeric, X_filtered_test_variance_new)\n",
    "# print(anova_selected_features_new)\n",
    "# print(X_filtered_test_anova_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering with mutual information: non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_mi(X_train, y_train, X_test, k=75, plot=True, random_state=42):\n",
    "    \"\"\"\n",
    "    Selects the top-k features using mutual information (for non-linear dependencies).\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series or np.array): Training labels, numerically encoded\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        k (int): Number of top features to select.\n",
    "        plot (bool): If True, plots MI scores and a 2D scatterplot of top 2 features.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        X_train_mi (pd.DataFrame): Training set with selected features.\n",
    "        X_test_mi (pd.DataFrame): Test set with selected features.\n",
    "        selected_features (List[str]): Names of the selected features.\n",
    "    \"\"\"\n",
    "    selector_mi = SelectKBest(lambda X, y: mutual_info_classif(X, y, random_state=random_state), k=k)\n",
    "    X_train_filtered_mi_np = selector_mi.fit_transform(X_train, y_train)\n",
    "    selected_features = X_train.columns[selector_mi.get_support()]\n",
    "\n",
    "    X_train_mi = pd.DataFrame(X_train_filtered_mi_np, columns=selected_features, index=X_train.index)\n",
    "    X_test_mi = pd.DataFrame(selector_mi.transform(X_test), columns=selected_features, index=X_test.index)\n",
    "\n",
    "    print(f\"[Mutual Information] Selected top-{k} features.\")\n",
    "    return X_train_mi, X_test_mi, selected_features\n",
    "\n",
    "#X_filtered_train_mi_new, X_filtered_test_mi_new, mi_selected_features_new = select_features_mi(X_filtered_train_variance_new, y_train_numeric, X_filtered_test_variance_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take union of ANOVA & MI selected sets: in this way we take linear and non-linear relations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feature_sets_union(X_full_train, X_full_test, features1, features2):\n",
    "    \"\"\"\n",
    "    Creates a feature subset based on the union of two feature lists.\n",
    "\n",
    "    Args:\n",
    "        X_full_train (pd.DataFrame): Original training DataFrame (after variance filtering).\n",
    "        X_full_test (pd.DataFrame): Original test DataFrame (after variance filtering).\n",
    "        features1 (List[str]): List of selected features (e.g., from ANOVA).\n",
    "        features2 (List[str]): List of selected features (e.g., from MI).\n",
    "\n",
    "    Returns:\n",
    "        X_union_train (pd.DataFrame): Training set with union of features.\n",
    "        X_union_test (pd.DataFrame): Test set with union of features.\n",
    "        union_features (List[str]): Combined list of feature names.\n",
    "    \"\"\"\n",
    "    union_features = list(set(features1).union(set(features2)))\n",
    "    X_union_train = X_full_train[union_features]\n",
    "    X_union_test = X_full_test[union_features]\n",
    "\n",
    "    print(f\"[Feature Union] Combined feature set size: {len(union_features)}\")\n",
    "    return X_union_train, X_union_test, union_features\n",
    "\n",
    "#X_filtered_train_union_new, X_filtered_test_union_new, union_features_new = combine_feature_sets_union(X_filtered_train_variance_new, X_filtered_test_variance_new, anova_selected_features_new, mi_selected_features_new)\n",
    "#print(len(union_features_new))\n",
    "#print(union_features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bijvoorbeeld op een paar features uit union:\n",
    "#for feature in union_features_new[:70]:  # check bijv. eerste 5 features\n",
    "    # sns.histplot(X_filtered_train_union_new[feature], kde=True)\n",
    "    # plt.title(f'Distribution of {feature}')\n",
    "    # plt.show()\n",
    "\n",
    "    #stat, p = stats.shapiro(X_filtered_train_union_new[feature])\n",
    "    #print(f\"{feature}: p={p} (Shapiro-Wilk Test)\")\n",
    "    #if p > 0.05:\n",
    "        #print(f\"Feature {feature} follows a normal distribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_forward_selection(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: Union[pd.Series, list, np.ndarray],\n",
    "    X_test: pd.DataFrame,\n",
    "    model=None,\n",
    "    feature_range: List[int] = None,\n",
    "    scoring: str = 'accuracy',\n",
    "    cv_splits: int = 5,\n",
    "    plot: bool = True\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, List[str], int, float]:\n",
    "    \"\"\"\n",
    "    Applies greedy forward feature selection with a given model.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (array-like): Training labels.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        model: Scikit-learn classifier (default: LogisticRegression).\n",
    "        feature_range (list): List of n_features_to_select to evaluate.\n",
    "        scoring (str): Scoring metric (e.g. 'accuracy', 'roc_auc').\n",
    "        cv_splits (int): Number of folds for cross-validation.\n",
    "        plot (bool): Whether to plot accuracy vs. feature count.\n",
    "\n",
    "    Returns:\n",
    "        X_train_selected (pd.DataFrame): Selected training features.\n",
    "        X_test_selected (pd.DataFrame): Selected test features.\n",
    "        best_features (List[str]): Names of selected features.\n",
    "        best_n_features (int): Number of selected features.\n",
    "        best_accuracy (float): Training accuracy using selected features.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "    if feature_range is None:\n",
    "        total_features = X_train.shape[1]\n",
    "        minimum_number_to_select = 10\n",
    "        maximum_number_to_select = len(total_features)\n",
    "        stepsize = 5\n",
    "        feature_range = list(range(minimum_number_to_select, maximum_number_to_select+1, stepsize))\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_n_features = None\n",
    "    best_features = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    for n_features in feature_range:\n",
    "        print(f\"Testing n_features_to_select = {n_features}\")\n",
    "\n",
    "        sfs = SequentialFeatureSelector(\n",
    "            model,\n",
    "            n_features_to_select=n_features,\n",
    "            direction='forward',\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        sfs.fit(X_train, y_train)\n",
    "        selected = list(X_train.columns[sfs.get_support()])\n",
    "\n",
    "        X_train_sel = X_train[selected]\n",
    "        X_test_sel = X_test[selected]\n",
    "\n",
    "        # new model per run\n",
    "        current_model = clone(model)\n",
    "        current_model.fit(X_train_sel, y_train)\n",
    "        acc = current_model.score(X_train_sel, y_train)\n",
    "        accuracy_list.append(acc)\n",
    "\n",
    "        print(f\"Accuracy for {n_features} features: {acc:.4f}\")\n",
    "\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_n_features = n_features\n",
    "            best_features = selected\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(feature_range, accuracy_list, marker='o')\n",
    "        plt.xlabel(\"Number of Features\")\n",
    "        plt.ylabel(f\"Training {scoring.capitalize()}\")\n",
    "        plt.title(\"Greedy Forward Selection Performance\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\n Greedy Forward Selection complete.\")\n",
    "    print(f\"Optimal number of features: {best_n_features}\")\n",
    "    print(f\"Best training {scoring}: {best_accuracy:.4f}\")\n",
    "\n",
    "    X_train_best = X_train[best_features]\n",
    "    X_test_best = X_test[best_features]\n",
    "\n",
    "    return X_train_best, X_test_best, best_features, best_n_features, best_accuracy\n",
    "\n",
    "# Feature selection for the Logistic Regression Classifier\n",
    "# Base estimator = Linear Regression\n",
    "# Optimize number of features between 10 and the number of features in the union\n",
    "#X_train_selected_sfs_logistic, X_test_selected_sfs_logistic, sfs_selected_features_logistic, number_of_sfs_selected_features_logistic, train_acc_logistic = greedy_forward_selection(\n",
    "    #X_filtered_train_union_new, y_train_numeric, X_filtered_test_union_new,\n",
    "    #model=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    #feature_range=list(range(80, 100, 5)),\n",
    "    #scoring='accuracy'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for the SVM classifier\n",
    "# Base estimator = Linear Regression\n",
    "# Optimize number of features with a max of 10\n",
    "#X_train_selected_sfs_logistic_max10, X_test_selected_sfs_logistic_max10, sfs_selected_features_logistic_max10, number_of_sfs_selected_features_logistic_max10, train_acc_logistic_max10 = greedy_forward_selection(\n",
    "    #X_filtered_train_union_new, y_train_numeric, X_filtered_test_union_new,\n",
    "    #model=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    #feature_range=list(range(1, 20, 1)),\n",
    "    #scoring='accuracy'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First optimize k with grid search before using kNN as base esimator \n",
    "\n",
    "def optimal_k(X_filtered_train_union_new, y_train_numeric):\n",
    "\n",
    "# Train tijdelijk op alle union features\n",
    "    X_temp = X_filtered_train_union_new\n",
    "\n",
    "    # parameter grid\n",
    "    param_grid = {'n_neighbors': list(range(1, 21))}\n",
    "\n",
    "    # Grid search with 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        KNeighborsClassifier(),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_temp, y_train_numeric)\n",
    "\n",
    "    # Get best k \n",
    "    best_k = grid_search.best_params_['n_neighbors']\n",
    "    print(f\"Best k found by grid search: {best_k}\")\n",
    "    print(f\"Cross-validated accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Gebruik best_k als base estimator\n",
    "    knn_best = KNeighborsClassifier(n_neighbors=best_k)\n",
    "\n",
    "    return knn_best\n",
    "\n",
    "#knn_best = optimal_k(X_filtered_train_union_new, y_train_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for the kNN classifier\n",
    "# Base estimator = kNN classifier\n",
    "# Optimize number of features between 1 and the number of features in the union\n",
    "#X_train_selected_sfs_knn, X_test_selected_sfs_knn, sfs_selected_features_knn, number_of_sfs_selected_features_knn, train_acc_knn = greedy_forward_selection(\n",
    "    #X_filtered_train_union_new, y_train_numeric, X_filtered_test_union_new,\n",
    "    #model=knn_best,\n",
    "    #feature_range=list(range(1, 21, 1)),\n",
    "    #scoring='accuracy'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sfs_selected_features_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further refining with rfecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfecv_refinement(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: Union[pd.Series, list, np.ndarray],\n",
    "    X_test: pd.DataFrame,\n",
    "    model=None,\n",
    "    min_features_to_select: int = 10,\n",
    "    step: int = 1,\n",
    "    scoring: str = 'accuracy',\n",
    "    cv_splits: int = 4,\n",
    "    plot: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies RFECV (recursive feature elimination with cross-validation) to refine a feature subset.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Feature-selected training set.\n",
    "        y_train (array-like): Labels.\n",
    "        X_test (pd.DataFrame): Test set (same columns as X_train).\n",
    "        model: Estimator to use (default: LogisticRegression).\n",
    "        min_features_to_select (int): Minimum number of features to retain.\n",
    "        step (int): How many features to remove at each iteration.\n",
    "        scoring (str): Scoring metric.\n",
    "        cv_splits (int): Number of CV folds.\n",
    "        plot (bool): Whether to plot performance vs. feature count.\n",
    "\n",
    "    Returns:\n",
    "        X_train_refined (pd.DataFrame): Refined training set.\n",
    "        X_test_refined (pd.DataFrame): Refined test set.\n",
    "        selected_features (List[str]): Names of retained features.\n",
    "        n_selected (int): Number of selected features.\n",
    "        score (float): Final training score.\n",
    "    \"\"\"\n",
    "\n",
    "    if model is None:\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "    rfecv = RFECV(\n",
    "        estimator=clone(model),\n",
    "        step=step,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        min_features_to_select=min_features_to_select\n",
    "    )\n",
    "\n",
    "    print(f\"\\n Running RFECV refinement...\")\n",
    "    rfecv.fit(X_train, y_train)\n",
    "\n",
    "    selected_features = X_train.columns[rfecv.get_support()]\n",
    "    X_train_refined = X_train[selected_features]\n",
    "    X_test_refined = X_test[selected_features]\n",
    "\n",
    "    # Score model on refined set\n",
    "    final_model = clone(model)\n",
    "    final_model.fit(X_train_refined, y_train)\n",
    "    score = final_model.score(X_train_refined, y_train)\n",
    "\n",
    "    if plot and hasattr(rfecv, 'grid_scores_'):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),\n",
    "                 rfecv.grid_scores_, marker='o')\n",
    "        plt.xlabel(\"Number of Features\")\n",
    "        plt.ylabel(f\"Cross-validated {scoring.capitalize()}\")\n",
    "        plt.title(\"RFECV Performance vs. Number of Features\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"\\n✅ RFECV selected {len(selected_features)} features\")\n",
    "    print(\"Top features:\", list(selected_features))\n",
    "    print(f\"{scoring.capitalize()} on training set after refinement: {score:.4f}\")\n",
    "\n",
    "    return X_train_refined, X_test_refined, list(selected_features), len(selected_features), score\n",
    "\n",
    "#X_train_refined_rfecv, X_test_refined_rfecv, refined_features, n_rfecv, train_score = rfecv_refinement(\n",
    "    #X_train_selected_sfs, y_train_numeric, X_test_selected_sfs,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_selected_final = X_train_selected_sfs\n",
    "#print(X_train_selected_final.shape)\n",
    "#X_test_selected_final = X_test_selected_sfs\n",
    "#print(X_test_selected_final.shape)\n",
    "#selected_features_final = sfs_selected_features\n",
    "#print(len(selected_features_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the categorical y-labels to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_numeric(y_train, y_test):\n",
    "    \"\"\"\n",
    "    Encodes categorical labels from `y_train` and `y_test` into numeric values using `LabelEncoder`.\n",
    "\n",
    "    This function applies `LabelEncoder` from `sklearn.preprocessing` to transform categorical target \n",
    "    labels into numerical format, which is essential for machine learning models that require numeric inputs.\n",
    "\n",
    "    Args:\n",
    "        y_train (pd.Series): The training labels.\n",
    "        y_test (pd.Series): The test labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - `y_train_numeric` (numpy.ndarray): The transformed numeric labels for the training set.\n",
    "            - `y_test_numeric` (numpy.ndarray): The transformed numeric labels for the test set.\n",
    "\n",
    "    Prints:\n",
    "        - A dictionary mapping numeric labels to their corresponding original class names.\n",
    "\n",
    "    Note:\n",
    "        The function assumes `y_train` and `y_test` contain categorical labels.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    y_train_numeric = le.fit_transform(y_train)\n",
    "    y_test_numeric = le.transform(y_test)\n",
    "\n",
    "    # Store the mapping of labels\n",
    "    legend_labels = le.classes_  # This saves the original class names\n",
    "    print(\"Label Mapping:\", {i: label for i, label in enumerate(legend_labels)})\n",
    "\n",
    "    return y_train_numeric, y_test_numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization based feature selection: LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_fs(X_scaled_robust_train, X_scaled_robust_test, y_train_numeric):\n",
    "    \"\"\"\n",
    "    Performs optimization based feature selection using Lasso regression.\n",
    "\n",
    "    This function applies Lasso regression with automatic hyperparameter tuning \n",
    "    (via `LassoCV`) to identify the most important features from the robust training data.\n",
    "    It selects features by penalizing less important coefficients, setting some to zero.\n",
    "    \n",
    "    Args:\n",
    "        X_scaled_robust_train (pd.DataFrame): \n",
    "            The robust training dataset with numerical features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - `X_train_selected` (np.ndarray): The reduced training dataset containing only selected features.\n",
    "            - `X_test_selected` (np.ndarray): The reduced test dataset containing only selected features.\n",
    "    \n",
    "    Prints:\n",
    "        - The best `alpha` value found through cross-validation.\n",
    "        - The names of the selected features.\n",
    "\n",
    "    Note:\n",
    "        - This function assumes `y_train_numeric` is globally defined.\n",
    "        - `SelectFromModel` is used to remove unimportant features based on the Lasso model.\n",
    "    \"\"\"\n",
    "    # Define LassoCV with cross-validation\n",
    "    n_alphas = 200\n",
    "    alphas = np.logspace(-4, 1, n_alphas)\n",
    "    random_state = 42 #  Using this will produce the same results everytime, 42 is along 0 the most popular choice\n",
    "    lasso_cv = LassoCV(cv=5, alphas=alphas, random_state=random_state, max_iter=10000) \n",
    "\n",
    "    # Fit LassoCV on training data\n",
    "    lasso_cv.fit(X_scaled_robust_train, y_train_numeric)\n",
    "\n",
    "    # Get the best alpha value\n",
    "    best_alpha = lasso_cv.alpha_\n",
    "\n",
    "    # Train final Lasso model with optimal alpha\n",
    "    lasso = Lasso(alpha=best_alpha, fit_intercept=False)\n",
    "    lasso.fit(X_scaled_robust_train, y_train_numeric)\n",
    "\n",
    "    # Select features\n",
    "    selector = SelectFromModel(lasso, prefit=True)\n",
    "    X_train_selected = selector.transform(X_scaled_robust_train)\n",
    "    X_test_selected = selector.transform(X_scaled_robust_test)\n",
    "\n",
    "    # Get selected feature indices and names\n",
    "    selected_features = np.where(selector.get_support())[0]\n",
    "    selected_feature_names = X_scaled_robust_train.columns[selected_features]\n",
    "    n_features = len(selected_features)\n",
    "    \n",
    "    return best_alpha, X_train_selected, X_test_selected, selected_feature_names, n_features\n",
    "\n",
    "# Capture the return values when calling the function\n",
    "#best_alpha, X_train_sel_lasso, X_test_sel_lasso, selected_feature_names, n_features = lasso_fs(X_scaled_robust_train, X_scaled_robust_test, y_train_numeric)\n",
    "#y_train_numeric, y_test_numeric = y_numeric(y_train, y_test)\n",
    "# Print the captured values in the global scope\n",
    "#print(f\"Best alpha found: {best_alpha}\")\n",
    "#print(\"Selected Features:\", selected_feature_names)\n",
    "#print(f\"N features found: {n_features}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_alpha, X_train_sel, X_test_sel, selected_feature_names, n_features = lasso_fs(X_scaled_robust_train, X_scaled_robust_test, y_train_numeric)\n",
    "\n",
    "def PCA_fs(X_scaled_robust_train, X_scaled_robust_test, target_variance=0.95):\n",
    "    \"\"\"\n",
    "    Performs Principal Component Analysis (PCA) for feature reduction.\n",
    "\n",
    "    This function applies PCA to reduce the dimensionality of the input data, \n",
    "    transforming the training and test datasets to a lower number of features \n",
    "    (specified by `n_features`), while retaining as much variance as possible.\n",
    "\n",
    "    Args:\n",
    "        n_features (int): \n",
    "            The number of principal components to retain after performing PCA.\n",
    "            Based on the number of features of Lasso.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - `X_scaled_robust_train_PCA` (np.ndarray): The transformed training dataset with reduced features.\n",
    "            - `X_scaled_robust_test_PCA` (np.ndarray): The transformed test dataset with reduced features.\n",
    "\n",
    "    Notes:\n",
    "        - The function assumes `X_scaled_robust_train` and `X_scaled_robust_test` are predefined globally.\n",
    "        - The function applies the same transformation to both the training and test datasets, \n",
    "          ensuring that the test set is projected into the same lower-dimensional space as the training set.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=target_variance)\n",
    "    X_train_PCA = pca.fit_transform(X_scaled_robust_train)\n",
    "    X_test_PCA = pca.transform(X_scaled_robust_test)  # Use the same transformation\n",
    "\n",
    "    # Determine how many components were selected\n",
    "    n_components_selected = pca.n_components_\n",
    "\n",
    "    print(f\"Selected {n_components_selected} components to retain {target_variance*100}% variance.\")\n",
    "    \n",
    "    return pca, X_train_PCA, X_test_PCA, n_components_selected\n",
    "\n",
    "# Assuming you already have the transformed PCA data\n",
    "#pca, X_train_pca, X_test_pca, n_components_selected = PCA_fs(X_scaled_robust_train, X_scaled_robust_test, target_variance=0.95)\n",
    "#best_alpha, X_train_sel, X_test_sel, selected_feature_names, n_features = lasso_fs(X_scaled_robust_train, X_scaled_robust_test, y_train_numeric)\n",
    "\n",
    "# Plot the cumulative variance versus the number of components\n",
    "#plt.figure(figsize=(8,5))\n",
    "#plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
    "#plt.axhline(y=0.95, color='r', linestyle='--', label=\"95% Variance\")\n",
    "#plt.axvline(x=n_components_selected, color='g', linestyle='--', label=f\"{n_components_selected} Components\")\n",
    "#plt.xlabel('Number of Principal Components')\n",
    "#plt.ylabel('Cumulative Explained Variance')\n",
    "#plt.title('Explained Variance vs. Number of Components')\n",
    "#plt.legend()\n",
    "#plt.grid()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing feature selections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Zet de labels om naar numerieke waarden\n",
    "# le = LabelEncoder()\n",
    "# y_train = le.fit_transform(y_train)  # Zet 'lipoma' en 'liposarcoma' om naar 0 en 1\n",
    "# y_test = le.transform(y_test)  # Pas dezelfde transformatie toe op y_test\n",
    "\n",
    "def test_lasso(X_train_sel_lasso, X_test_sel_lasso, y_test_numeric):\n",
    "\n",
    "    # Define the classifiers\n",
    "    classifiers = {\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=500, random_state=42),\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "        \"kNN\": KNeighborsClassifier(n_neighbors=5)  # Je kunt het aantal neighbors aanpassen\n",
    "    }\n",
    "\n",
    "    # cross-validation setup\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Store fitted classifiers\n",
    "    clfs_fit = []\n",
    "\n",
    "    # Select the same features for the test set\n",
    "    X_train_selected = X_train_sel_lasso\n",
    "    X_test_selected = X_test_sel_lasso\n",
    "    print(X_test_selected.shape)\n",
    "    print(y_test_numeric.shape)\n",
    "\n",
    "    # Train and evaluate classifiers\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        # Train classifier\n",
    "        # clf.fit(X_train_selected, y_train_numeric)\n",
    "        Y_pred = cross_val_predict(clf, X_train_selected, y_train_numeric, cv=cv)\n",
    "\n",
    "        # Store fitted classifier\n",
    "        clfs_fit.append(copy.deepcopy(clf))\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        accuracy = metrics.accuracy_score(y_train_numeric, Y_pred)\n",
    "\n",
    "        # Handle the case where we can get probabilities\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            # Get the probability for class 1 (index 1)\n",
    "            y_score = cross_val_predict(clf, X_train_selected, y_train_numeric, cv=cv, method='predict_proba')[:, 1]\n",
    "        else:\n",
    "            # If predict_proba is not available, use the binary predictions\n",
    "            y_score = Y_pred  # Use binary predictions if probability is unavailable\n",
    "\n",
    "        auc = metrics.roc_auc_score(y_train_numeric, y_score)\n",
    "        f1 = metrics.f1_score(y_train_numeric, Y_pred)\n",
    "        precision = metrics.precision_score(y_train_numeric, Y_pred)\n",
    "        recall = metrics.recall_score(y_train_numeric, Y_pred)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Classifier: {clf.__class__.__name__}\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  AUC: {auc:.4f}\")\n",
    "        print(f\"  F1-Score: {f1:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "#test_lasso(X_train_sel_lasso, X_test_sel_lasso, y_test_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Zet de labels om naar numerieke waarden\n",
    "# le = LabelEncoder()\n",
    "# y_train = le.fit_transform(y_train)  # Zet 'lipoma' en 'liposarcoma' om naar 0 en 1\n",
    "# y_test = le.transform(y_test)  # Pas dezelfde transformatie toe op y_test\n",
    "\n",
    "def test_pca(X_train_pca, X_test_pca, y_train_numeric):\n",
    "\n",
    "    # Define the classifiers\n",
    "    classifiers = {\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=500, random_state=42),\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "        \"kNN\": KNeighborsClassifier(n_neighbors=5)  # Je kunt het aantal neighbors aanpassen\n",
    "    }\n",
    "\n",
    "    # cross-validation setup\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Store fitted classifiers\n",
    "    clfs_fit = []\n",
    "\n",
    "    # Select the same features for the test set\n",
    "    X_train_selected_pca = X_train_pca\n",
    "    X_test_selected_pca = X_test_pca\n",
    "    print(X_test_selected_pca.shape)\n",
    "\n",
    "    # Train and evaluate classifiers\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        # Train classifier\n",
    "        # clf.fit(X_train_selected, y_train_numeric)\n",
    "        Y_pred = cross_val_predict(clf, X_train_selected_pca, y_train_numeric, cv=cv)\n",
    "\n",
    "        # Store fitted classifier\n",
    "        clfs_fit.append(copy.deepcopy(clf))\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        accuracy = metrics.accuracy_score(y_train_numeric, Y_pred)\n",
    "\n",
    "        # Handle the case where we can get probabilities\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            # Get the probability for class 1 (index 1)\n",
    "            y_score = cross_val_predict(clf, X_train_selected_pca, y_train_numeric, cv=cv, method='predict_proba')[:, 1]\n",
    "        else:\n",
    "            # If predict_proba is not available, use the binary predictions\n",
    "            y_score = Y_pred  # Use binary predictions if probability is unavailable\n",
    "\n",
    "        auc = metrics.roc_auc_score(y_train_numeric, y_score)\n",
    "        f1 = metrics.f1_score(y_train_numeric, Y_pred)\n",
    "        precision = metrics.precision_score(y_train_numeric, Y_pred)\n",
    "        recall = metrics.recall_score(y_train_numeric, Y_pred)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Classifier: {clf.__class__.__name__}\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  AUC: {auc:.4f}\")\n",
    "        print(f\"  F1-Score: {f1:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "#test_pca(X_train_pca, X_test_pca, y_train_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "Random forest, decision tree and bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_tree_classifiers(y_train, X_train):\n",
    "\n",
    "    # Encode categorical labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "    # Define models and parameter distributions\n",
    "    models = {\n",
    "        \"Random Forest\": (RandomForestClassifier(random_state=42), {\n",
    "            'n_estimators': randint(5, 200),\n",
    "            'max_depth': randint(3, 20),\n",
    "            'min_samples_split': randint(5, 20),\n",
    "            'bootstrap': [True, False]\n",
    "        }),\n",
    "        \"Bagging\": (BaggingClassifier(DecisionTreeClassifier(random_state=42), random_state=42), {\n",
    "            'n_estimators': randint(5, 200),\n",
    "            'estimator__max_depth': randint(3, 5),\n",
    "            'estimator__min_samples_split': randint(5, 20),\n",
    "            'bootstrap': [True, False]\n",
    "        }),\n",
    "        \"Decision Tree\": (DecisionTreeClassifier(random_state=42), {\n",
    "            'max_depth': randint(3, 5),\n",
    "            'min_samples_split': randint(5, 20)\n",
    "        })\n",
    "    }\n",
    "\n",
    "    # Perform Randomized Search and store results\n",
    "    best_estimators = {}\n",
    "    best_params = {}\n",
    "    best_scores = {}\n",
    "\n",
    "    for name, (model, param_dist) in models.items():\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=30,\n",
    "            scoring='accuracy',\n",
    "            cv=10,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        search.fit(X_train, y_train_encoded) # Fitted on data without scaling and feature selection\n",
    "\n",
    "        best_estimators[name] = search.best_estimator_\n",
    "        best_params[name] = search.best_params_\n",
    "        best_scores[name] = search.best_score_\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n=== Model Comparison ===\")\n",
    "    for model_name in models.keys():\n",
    "        print(f\"\\nBest {model_name}: {best_estimators[model_name]}\")\n",
    "        print(f\"Best {model_name} Parameters: {best_params[model_name]}\")\n",
    "        print(f\"Best {model_name} Accuracy: {best_scores[model_name]:.4f}\")\n",
    "\n",
    "    return best_estimators, y_train_encoded, best_scores\n",
    "\n",
    "#best_estimators, y_train_encoded, best_scores = forest_tree_classifiers(y_train, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier(X_train_selected_final, y_train_encoded):\n",
    "\n",
    "    # Define base SVM model\n",
    "    svm_model = SVC(probability=True, random_state=42)\n",
    "\n",
    "    # Define parameter distribution for RandomizedSearchCV\n",
    "    param_dist = {\n",
    "        'C': [0.1, 1, 10, 100, 1000],  # Regularization parameter\n",
    "        'gamma': ['scale', 'auto', 0.1, 1, 10, 100],  # Kernel coefficient\n",
    "        'kernel': ['linear', 'rbf', 'poly'],  # Different kernel types\n",
    "        'degree': [1, 2, 3, 4, 5, 6],  # Only used for poly kernel\n",
    "    }\n",
    "\n",
    "    # Randomized Search with cv=10\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=svm_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,  # More iterations for better results\n",
    "        scoring='accuracy',\n",
    "        cv=10,  # 10-fold cross-validation\n",
    "        n_jobs=-1,  # Use all CPU cores\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on robustly scaled training data\n",
    "    random_search.fit(X_train_selected_final, y_train_encoded)\n",
    "\n",
    "    # Get the best model and parameters\n",
    "    best_svm = random_search.best_estimator_\n",
    "    best_params_svm = random_search.best_params_\n",
    "    best_score_svm = random_search.best_score_\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n=== Best SVM Model After Randomized Search ===\")\n",
    "    print(f\"Best SVM: {best_svm}\")\n",
    "    print(f\"Best SVM Parameters: {best_params_svm}\")\n",
    "    print(f\"Best SVM Accuracy: {best_score_svm:.4f}\")\n",
    "\n",
    "    return best_svm, best_score_svm\n",
    "\n",
    "#best_svm, best_score_svm = svm_classifier(X_train_selected_sfs_logistic_max10, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(X_train_selected_final, y_train_encoded):\n",
    "\n",
    "    # Define base kNN model\n",
    "    knn_model = KNeighborsClassifier()\n",
    "\n",
    "    # Define parameter distribution for RandomizedSearchCV\n",
    "    param_dist_knn = {\n",
    "        'n_neighbors': randint(3, 50),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size': randint(20, 60),\n",
    "        'metric': ['minkowski', 'euclidean', 'manhattan']\n",
    "    }\n",
    "\n",
    "    cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    random_search_knn = RandomizedSearchCV(\n",
    "        estimator=knn_model,\n",
    "        param_distributions=param_dist_knn,\n",
    "        n_iter=30,\n",
    "        scoring='accuracy',\n",
    "        cv=cv_strategy,  \n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit op geschaalde data\n",
    "    random_search_knn.fit(X_train_selected_final, y_train_encoded)\n",
    "\n",
    "    # Beste model, parameters, en score ophalen\n",
    "    best_knn = random_search_knn.best_estimator_\n",
    "    best_params_knn = random_search_knn.best_params_\n",
    "    best_score_knn = random_search_knn.best_score_\n",
    "\n",
    "    # Resultaten afdrukken\n",
    "    print(\"\\n=== Best kNN Model After Randomized Search ===\")\n",
    "    print(f\"Best kNN: {best_knn}\")\n",
    "    print(f\"Best kNN Parameters: {best_params_knn}\")\n",
    "    print(f\"Best kNN Accuracy: {best_score_knn:.4f}\")\n",
    "\n",
    "    return best_knn, best_score_knn\n",
    "\n",
    "#best_knn, best_score_knn = knn_classifier(X_train_selected_sfs_knn, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_classifier(X_train_selected_final, y_train_encoded):\n",
    "\n",
    "    # Function to perform cross-validation and get accuracy\n",
    "    def perform_cross_val(model, X, y, cv=10):\n",
    "        return np.mean(cross_val_score(model, X, y, cv=cv, scoring='accuracy'))\n",
    "\n",
    "    # Perform cross-validation for Logistic Regression with default parameters\n",
    "    log_reg_model = LogisticRegression(max_iter=5000)\n",
    "    log_reg_score = perform_cross_val(log_reg_model, X_train_selected_final, y_train_encoded)\n",
    "    print(f\"\\nAccuracy for Logistic Regression: {log_reg_score:.4f}\")\n",
    "\n",
    "    return log_reg_score, log_reg_model\n",
    "\n",
    "#log_reg_score, log_reg_model = lr_classifier(X_train_selected_sfs_logistic, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curves(best_estimators, X_train, y_train_encoded, best_knn, X_train_selected_sfs_logistic, X_train_selected_sfs_logistic_max10, X_train_selected_sfs_knn, log_reg_model):\n",
    "\n",
    "    # Create subplots (2 rows, 2 columns)\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(14, 15))\n",
    "\n",
    "    # Plot learning curves for each best model\n",
    "    #plot_learning_curve(best_svm, \"SVM Learning Curve\", X_train_selected_sfs_logistic_max10, y_train_encoded, axes[0, 0], cv=10)\n",
    "    plot_learning_curve(best_estimators[\"Random Forest\"], \"Random Forest Learning Curve\", X_train, y_train_encoded, axes[0, 1], cv=10)\n",
    "    plot_learning_curve(best_estimators[\"Bagging\"], \"Bagging Classifier Learning Curve\", X_train, y_train_encoded, axes[1, 0], cv=10)\n",
    "    plot_learning_curve(best_estimators[\"Decision Tree\"], \"Decision Tree Learning Curve\", X_train, y_train_encoded, axes[1, 1], cv=10)\n",
    "    plot_learning_curve(best_knn, \"kNN Learning Curve\", X_train_selected_sfs_knn, y_train_encoded, axes[2, 0], cv=10)\n",
    "    plot_learning_curve(log_reg_model, \"Linear Regression Learning Curve\", X_train_selected_sfs_logistic, y_train_encoded, axes[2, 1], cv=10)\n",
    "\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#learning_curves(best_estimators, X_train, y_train_encoded, best_knn, X_train_selected_sfs_logistic, X_train_selected_sfs_logistic_max10, X_train_selected_sfs_knn, log_reg_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_classifiers(best_scores, best_score_knn, X_scaled_robust_train, X_scaled_robust_test, y_train_encoded, best_knn, X_train_selected_sfs_logistic, X_test_selected_sfs_logistic, X_train_selected_sfs_logistic_max10, X_test_selected_sfs_logistic_max10, X_train_selected_sfs_knn, X_test_selected_sfs_knn, y_test, y_train, log_reg_score, best_estimators):\n",
    "\n",
    "    # Store model names and best accuracy scores\n",
    "    model_scores = {\n",
    "        \"Random Forest\": best_scores[\"Random Forest\"],\n",
    "        \"Bagging\": best_scores[\"Bagging\"],\n",
    "        \"Decision Tree\": best_scores[\"Decision Tree\"],\n",
    "        #\"SVM\": best_score_svm,\n",
    "        #\"kNN\": best_score_knn,\n",
    "        \"Logistic Regression\": log_reg_score\n",
    "    }\n",
    "\n",
    "    # Identify the best model\n",
    "    best_model_name = max(model_scores, key=model_scores.get)\n",
    "    print(f\"\\nBest Overall Model: {best_model_name}\")\n",
    "\n",
    "    # Determine the correct dataset and fit the best model\n",
    "    if best_model_name in best_estimators:  # Models trained on X_scaled_robust_train\n",
    "        best_model = best_estimators[best_model_name]\n",
    "        best_model.fit(X_scaled_robust_train, y_train_encoded)\n",
    "        y_score = best_model.predict_proba(X_scaled_robust_test)\n",
    "        y_pred = best_model.predict(X_scaled_robust_test)\n",
    "        \n",
    "    elif best_model_name == \"SVM\":\n",
    "        best_model = best_svm\n",
    "        best_model.fit(X_train_selected_sfs_logistic_max10, y_train_encoded)\n",
    "        y_score = best_model.predict_proba(X_test_selected_sfs_logistic_max10)  # Use same subset of features\n",
    "        y_pred = best_model.predict(X_test_selected_sfs_logistic_max10)\n",
    "\n",
    "    elif best_model_name == \"kNN\":\n",
    "        best_model = best_knn\n",
    "        best_model.fit(X_train_selected_sfs_knn, y_train_encoded)\n",
    "        y_score = best_model.predict_proba(X_test_selected_sfs_knn)\n",
    "        y_pred = best_model.predict(X_test_selected_sfs_knn)\n",
    "\n",
    "    elif best_model_name == \"Logistic Regression\":\n",
    "        best_model = LogisticRegression(max_iter=5000)\n",
    "        best_model.fit(X_train_selected_sfs_logistic, y_train_encoded)\n",
    "        y_score = best_model.predict_proba(X_test_selected_sfs_logistic)\n",
    "        y_pred = best_model.predict(X_test_selected_sfs_logistic)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nFinal Model: {best_model_name}\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "    f1 = f1_score(y_test_encoded, y_pred)\n",
    "\n",
    "    # Print the accuracy and F1 score\n",
    "    print(f\"Accuracy on the Test Set: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score on the Test Set: {f1:.4f}\")\n",
    "\n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, auc_value = plot_roc_curve(y_score, y_test_encoded)\n",
    "\n",
    "    return fpr, tpr, auc_value\n",
    "\n",
    "#fpr, tpr, auc_value = evaluation_classifiers(best_scores, best_score_knn, X_scaled_robust_train, X_scaled_robust_test, y_train_encoded, best_knn, X_train_selected_sfs_logistic, X_test_selected_sfs_logistic, X_train_selected_sfs_logistics_max10, X_test_selected_sfs_logistic_max10, X_train_selected_sfs_knn, X_test_selected_sfs_knn, y_test, y_train, log_reg_score, best_estimators)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "all_fpr = []\n",
    "all_tpr = []\n",
    "all_auc = []\n",
    "\n",
    "# Loop over the folds\n",
    "for validation_index, test_index in cv_5fold.split(X, y):\n",
    "    # Split the data properly\n",
    "    print('=' * 30 + 'START ONE FOLD' + '=' * 30)\n",
    "    X_train = X.iloc[validation_index]\n",
    "    y_train = y.iloc[validation_index]\n",
    "\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "    # Missing data\n",
    "    total_missing = find_missing_data(X_train)\n",
    "    X_train, X_test = process_missing_data(X_train, X_test, total_missing)\n",
    "\n",
    "    # Scaling\n",
    "    X_scaled_robust_train, X_scaled_robust_test = scaling(X_train, X_test)\n",
    "\n",
    "    print('-' * 30 + 'Start feature selection' + '-' * 30)\n",
    "\n",
    "    # Feature selection\n",
    "    y_train_numeric, y_test_numeric = y_numeric(y_train, y_test)\n",
    "    X_filtered_train_variance_new, X_filtered_test_variance_new, variance_filtered_features_new = apply_variance_threshold(X_scaled_robust_train, X_scaled_robust_test)\n",
    "    X_filtered_train_anova_new, X_filtered_test_anova_new, anova_selected_features_new = select_features_anova(X_filtered_train_variance_new, y_train_numeric, X_filtered_test_variance_new)\n",
    "    X_filtered_train_mi_new, X_filtered_test_mi_new, mi_selected_features_new = select_features_mi(X_filtered_train_variance_new, y_train_numeric, X_filtered_test_variance_new)\n",
    "    X_filtered_train_union_new, X_filtered_test_union_new, union_features_new = combine_feature_sets_union(X_filtered_train_variance_new, X_filtered_test_variance_new, anova_selected_features_new, mi_selected_features_new)\n",
    "    \n",
    "    X_train_selected_sfs_logistic, X_test_selected_sfs_logistic, sfs_selected_features_logistic, number_of_sfs_selected_features_logistic, train_acc_logistic = greedy_forward_selection(\n",
    "    X_filtered_train_union_new, y_train_numeric, X_filtered_test_union_new,\n",
    "    model=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    feature_range=[90],\n",
    "    scoring='accuracy'\n",
    "    )\n",
    "\n",
    "    X_train_selected_sfs_logistic_max10, X_test_selected_sfs_logistic_max10, sfs_selected_features_logistic_max10, number_of_sfs_selected_features_logistic_max10, train_acc_logistic_max10 = greedy_forward_selection(\n",
    "    X_filtered_train_union_new, y_train_numeric, X_filtered_test_union_new,\n",
    "    model=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    feature_range=[14],\n",
    "    scoring='accuracy'\n",
    "    )\n",
    "\n",
    "    knn_best = optimal_k(X_filtered_train_union_new, y_train_numeric)\n",
    "\n",
    "    X_train_selected_sfs_knn, X_test_selected_sfs_knn, sfs_selected_features_knn, number_of_sfs_selected_features_knn, train_acc_knn = greedy_forward_selection(\n",
    "    X_filtered_train_union_new, y_train_numeric, X_filtered_test_union_new,\n",
    "    model=knn_best,\n",
    "    feature_range=[7],\n",
    "    scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    # Capture the return values when calling the function\n",
    "    best_alpha, X_train_sel_lasso, X_test_sel_lasso, selected_feature_names, n_features = lasso_fs(X_scaled_robust_train, X_scaled_robust_test, y_train_numeric)\n",
    "    best_alpha, X_train_sel, X_test_sel, selected_feature_names, n_features = lasso_fs(X_scaled_robust_train, X_scaled_robust_test, y_train_numeric)    \n",
    "    # Print the captured values in the global scope\n",
    "    print(f\"Best alpha found: {best_alpha}\")\n",
    "    print(\"Selected Features:\", selected_feature_names)\n",
    "    print(f\"N features found: {n_features}\")\n",
    "\n",
    "    pca, X_train_pca, X_test_pca, n_components_selected = PCA_fs(X_scaled_robust_train, X_scaled_robust_test, target_variance=0.95)\n",
    "    test_lasso(X_train_sel_lasso, X_test_sel_lasso, y_test_numeric)\n",
    "    test_pca(X_train_pca, X_test_pca, y_train_numeric)\n",
    "\n",
    "    print('-' * 30 + 'Start training classifiers' + '-' * 30)\n",
    "\n",
    "    # Classifiers\n",
    "    best_estimators, y_train_encoded, best_scores = forest_tree_classifiers(y_train, X_train)\n",
    "    #best_svm, best_score_svm = svm_classifier(X_train_selected_sfs_logistic_max10, y_train_encoded)\n",
    "    best_knn, best_score_knn = knn_classifier(X_train_selected_sfs_knn, y_train_encoded)\n",
    "    log_reg_score, log_reg_model = lr_classifier(X_train_selected_sfs_logistic, y_train_encoded)\n",
    "\n",
    "    # Learning curves\n",
    "    learning_curves(best_estimators, X_train, y_train_encoded, best_knn, X_train_selected_sfs_logistic, X_train_selected_sfs_logistic_max10, X_train_selected_sfs_knn, log_reg_model)\n",
    "\n",
    "    # Evaluation\n",
    "    fpr, tpr, auc_value = evaluation_classifiers(best_scores, best_score_knn, X_scaled_robust_train, X_scaled_robust_test, y_train_encoded, best_knn, X_train_selected_sfs_logistic, X_test_selected_sfs_logistic, X_train_selected_sfs_logistic_max10, X_test_selected_sfs_logistic_max10, X_train_selected_sfs_knn, X_test_selected_sfs_knn, y_test, y_train, log_reg_score, best_estimators)\n",
    "\n",
    "    all_fpr.append(fpr)\n",
    "    all_tpr.append(tpr)\n",
    "    all_auc.append(auc_value)\n",
    "\n",
    "    print('=' * 30 + 'END ONE FOLD' + '=' * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot combined ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(all_fpr)):\n",
    "    plt.plot(all_fpr[i], all_tpr[i], label=f'Fold {i+1} (AUC = {all_auc[i]:.2f})')\n",
    "    \n",
    "# Plot a diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for 5-fold Cross Validation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
