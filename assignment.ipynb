{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "#from worcgist.load_data import load_data\n",
    "from worclipo.load_data import load_data\n",
    "#from worcliver.load_data import load_data\n",
    "#from ecg.load_data import load_data\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')\n",
    "\n",
    "X = data.drop(\"label\",axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    X_train_fold = X_train.iloc[train_idx]\n",
    "    y_train_fold = y_train.iloc[train_idx]\n",
    "    X_val_fold = X_train.iloc[val_idx]\n",
    "    y_val_fold = y_train.iloc[val_idx]\n",
    "\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\" - Train size: {len(X_train_fold)}\")\n",
    "    print(f\" - Validation size: {len(X_val_fold)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General functions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions we will use\n",
    "def colorplot(clf, ax, x, y, h=100, precomputer=None):\n",
    "    '''\n",
    "    Overlay the decision areas as colors in an axes.\n",
    "\n",
    "    Input:\n",
    "        clf: trained classifier\n",
    "        ax: axis to overlay color mesh on\n",
    "        x: feature on x-axis\n",
    "        y: feature on y-axis\n",
    "        h(optional): steps in the mesh\n",
    "    '''\n",
    "    # Create a meshgrid the size of the axis\n",
    "    xstep = (x.max() - x.min() ) / 20.0\n",
    "    ystep = (y.max() - y.min() ) / 20.0\n",
    "    x_min, x_max = x.min() - xstep, x.max() + xstep\n",
    "    y_min, y_max = y.min() - ystep, y.max() + ystep\n",
    "    h = max((x_max - x_min, y_max - y_min))/h\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    features = np.c_[xx.ravel(), yy.ravel()]\n",
    "    if precomputer is not None:\n",
    "        if type(precomputer) is RBFSampler:\n",
    "            features = precomputer.transform(features)\n",
    "        elif precomputer is rbf_kernel:\n",
    "            features = rbf_kernel(features, X)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(features)\n",
    "    else:\n",
    "        Z = clf.predict_proba(features)\n",
    "    if len(Z.shape) > 1:\n",
    "        Z = Z[:, 1]\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    cm = plt.cm.RdBu_r\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "    del xx, yy, x_min, x_max, y_min, y_max, Z, cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data\n",
    "* Part 1: Finding missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define missing value indicators\n",
    "custom_missing = ['NA', 'N/A', '?', 'None', 'none', '-']\n",
    "\n",
    "# Count NaNs\n",
    "nan_counts = X_train.isna().sum()\n",
    "\n",
    "# Count empty strings\n",
    "empty_string_counts = (X_train == '').sum()\n",
    "\n",
    "# Count custom missing indicators (case-insensitive match)\n",
    "custom_missing_counts = X_train.apply(lambda col: col.astype(str).str.lower().isin([val.lower() for val in custom_missing]).sum())\n",
    "\n",
    "# Compute total missing count per column\n",
    "total_missing = nan_counts + empty_string_counts + custom_missing_counts\n",
    "\n",
    "# Filter out columns where total missing is zero\n",
    "total_missing_selected = total_missing[total_missing != 0]\n",
    "\n",
    "# Print total missing counts\n",
    "print(total_missing_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Part 2: Processing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Replacing missing values with NaN\n",
    "custom_missing = ['NA', 'N/A', '?', 'None', 'none', '-', '']\n",
    "X_train.replace(custom_missing, np.nan, inplace=True)\n",
    "X_test.replace(custom_missing, np.nan, inplace=True)\n",
    "\n",
    "# If 50% or more of the data within one feature is missing the feature is deleted\n",
    "limit = len(X_train.index)*50/100\n",
    "valid_columns = [col for col, count in total_missing.items() if count < limit]\n",
    "\n",
    "# Keep only the valid columns in both X_train and X_test\n",
    "X_train = X_train[valid_columns]\n",
    "X_test = X_test[valid_columns]\n",
    "\n",
    "# Imputate \n",
    "\n",
    "# Check if imputation is needed\n",
    "if X_train.isna().sum().sum() == 0:\n",
    "    pass\n",
    "else:\n",
    "    # Dictionary to store mean/median decision per column\n",
    "    imputation_strategies = {}\n",
    "\n",
    "    for col in X_train.select_dtypes(include=['number']).columns:  # Only numeric columns\n",
    "        col_data = X_train[col].dropna()  # Remove NaN values for testing\n",
    "\n",
    "        if len(col_data) > 3:  # Shapiro requires at least 3 non-null values\n",
    "            if col_data.nunique() == 1:  # Check if all values are the same\n",
    "                strategy = 'median'  # Default to median if no variability\n",
    "            else:\n",
    "                _, p = shapiro(col_data)\n",
    "                strategy = 'mean' if p > 0.05 else 'median'\n",
    "        else:\n",
    "            strategy = 'median'  # Default to median if too few values\n",
    "\n",
    "        imputation_strategies[col] = strategy\n",
    "\n",
    "    # Create imputers for mean and median\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Apply imputers for each feature\n",
    "    for col, strategy in imputation_strategies.items():\n",
    "        imputer = mean_imputer if strategy == 'mean' else median_imputer\n",
    "        X_train[col] = imputer.fit_transform(X_train[[col]])\n",
    "        X_test[col] = imputer.transform(X_test[[col]])  # Use the same imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler_robust = preprocessing.RobustScaler()\n",
    "\n",
    "scaled_robust_array_train = scaler_robust.fit_transform(X_train)\n",
    "scaled_robust_array_test = scaler_robust.transform(X_test)\n",
    "\n",
    "X_scaled_robust_train = pd.DataFrame(scaled_robust_array_train, columns=X_train.columns)\n",
    "X_scaled_robust_test = pd.DataFrame(scaled_robust_array_test, columns=X_test.columns)\n",
    "\n",
    "# print(X_scaled_robust_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lloyd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rfecv en svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination with Cross-Validation (RFECV)\n",
    "\n",
    "# Import libraries\n",
    "from sklearn import feature_selection\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# calling the classifier (SVM model) for the feature selection\n",
    "svc = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "# RFECV feature selection and fitting\n",
    "rfecv = feature_selection.RFECV(\n",
    "    estimator=svc, step=5,\n",
    "    cv=model_selection.StratifiedKFold(n_splits=2, shuffle=True, random_state=42),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "# print selected feature names\n",
    "selected_features = X_train.columns[sfs1.get_support()]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# plot the number of features vs. accuracy\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean test accuracy\")\n",
    "plt.plot(range(1, len(rfecv.cv_results_[\"mean_test_score\"]) + 1), rfecv.cv_results_[\"mean_test_score\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate testing (ANOVA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Zet de labels om naar numerieke waarden\n",
    "le = LabelEncoder()\n",
    "y_train_numeric = le.fit_transform(y_train)\n",
    "legend_labels = le.classes_  # Toont de mapping van labels naar nummers\n",
    "\n",
    "# ANOVA F-test (lineaire relaties)\n",
    "selector_anova = SelectKBest(f_classif, k=40)\n",
    "X_filtered_anova = selector_anova.fit_transform(X_scaled_robust_train, y_train)\n",
    "anova_features = X_scaled_robust_train.columns[selector_anova.get_support()]\n",
    "# print(anova_features)\n",
    "f_values, p_values = selector_anova.scores_, selector_anova.pvalues_\n",
    "\n",
    "# Create a DataFrame to display features with their corresponding F-statistics and p-values\n",
    "anova_results = pd.DataFrame({'Feature': X_scaled_robust_train.columns,'F-Statistic': f_values, 'p-value': p_values})\n",
    "\n",
    "# Sort by F-statistic (highest to lowest) to find the most statistically significant features\n",
    "anova_results_sorted_by_F = anova_results.sort_values(by='F-Statistic', ascending=False)\n",
    "anova_results_sorted_by_F = anova_results_sorted_by_F[anova_results_sorted_by_F['Feature'].isin(anova_features)]\n",
    "print(anova_results_sorted_by_F)\n",
    "\n",
    "# Plot de F-statistic per feature\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(1, len(anova_results_sorted_by_F) + 1), anova_results_sorted_by_F['F-Statistic'], marker='o')\n",
    "plt.xlabel(\"Feature Rank (Sorted by F-Statistic)\")\n",
    "plt.ylabel(\"F-Statistic Value\")\n",
    "plt.title(\"F-Statistic vs. Ranked Features (ANOVA)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Mutual Information (niet-lineaire relaties)\n",
    "selector_mi = SelectKBest(mutual_info_classif, k=100)\n",
    "X_filtered_mi = selector_mi.fit_transform(X_scaled_robust_train, y_train)\n",
    "mi_features = X_scaled_robust_train.columns[selector_mi.get_support()]\n",
    "\n",
    "# Gemeenschappelijke features\n",
    "common_features = list(set(anova_features).intersection(set(mi_features)))\n",
    "\n",
    "# Unieke features voor ANOVA en MI\n",
    "unique_anova = list(set(anova_features) - set(mi_features))\n",
    "unique_mi = list(set(mi_features) - set(anova_features))\n",
    "\n",
    "# Print de resultaten\n",
    "print(\"Selected features using ANOVA:\", anova_features)\n",
    "print(\"Selected features using Mutual Information:\", mi_features)\n",
    "print(\"Common features (selected by both ANOVA and MI):\", common_features)\n",
    "print(\"Unique features to ANOVA:\", unique_anova)\n",
    "print(\"Unique features to MI:\", unique_mi)\n",
    "print(len(common_features))\n",
    "\n",
    "# fig = plt.figure(figsize=(24,8))\n",
    "# ax = fig.add_subplot(131)\n",
    "# ax.set_title(\"First two features of ANOVA test\", fontsize='small')\n",
    "# scatter = ax.scatter(X_filtered_anova[:, 0], X_filtered_anova[:, 2], marker='o', c=y_train_numeric,\n",
    "#             s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "# # Haal de juiste legenda-elementen op\n",
    "# handles, _ = scatter.legend_elements()\n",
    "# legend1 = ax.legend(handles, legend_labels, title=\"classes\")\n",
    "# ax.add_artist(legend1)\n",
    "# plt.show()\n",
    "\n",
    "example_features = anova_results_sorted_by_F.iloc[0:2]['Feature'].values\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.scatter(X_train[example_features[0]], X_train[example_features[1]], c=y_train_numeric, cmap=plt.cm.Paired, edgecolor='k')\n",
    "plt.xlabel(example_features[0])\n",
    "plt.ylabel(example_features[1])\n",
    "plt.title(f\"Scatter plot of 2 features: {example_features[0]} vs {example_features[1]}\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy feature selection, forward AFTER ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlxtend\n",
    "\n",
    "# Greedy feature selection (forward)\n",
    "\n",
    "# importing the models\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# calling the classifier for the feature selction\n",
    "clf = LogisticRegression(max_iter=500, random_state=42)\n",
    "svc = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "# Stratified K-Fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Greedy forward selection and fitting (select 10 best features)\n",
    "sfs1 = SequentialFeatureSelector(clf, n_features_to_select=10, direction='forward', scoring='accuracy', cv=cv)\n",
    "sfs1.fit(X_train, y_train)\n",
    "\n",
    "# print selected feature names\n",
    "selected_features = X_train.columns[sfs1.get_support()]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply RFECV/SFS after ANOVA selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Feature Selection with RFE\n",
    "\n",
    "# import modules\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "# calling the classifier (SVM model or linear regression) for the feature selection\n",
    "svc = svm.SVC(kernel=\"linear\")\n",
    "# clf = LogisticRegression(max_iter=500, random_state=42)\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# RFECV feature selection and fitting\n",
    "rfecv_anova = feature_selection.RFECV(estimator=svc,\n",
    "                                      step=5,\n",
    "                                      cv=cv,\n",
    "                                      scoring='roc_auc')\n",
    "\n",
    "rfecv_mi = feature_selection.RFECV(estimator=svc,\n",
    "                                   step=5,\n",
    "                                   cv=cv,\n",
    "                                   scoring='roc_auc')\n",
    "\n",
    "rfecv_anova.fit(X_filtered_anova, y_train)\n",
    "rfecv_mi.fit(X_filtered_mi, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print selected feature names and the number of features selected\n",
    "selected_features_anova = anova_features[rfecv_anova.support_]\n",
    "selected_features_mi = mi_features[rfecv_mi.support_]\n",
    "\n",
    "print(\"Features selected with ANOVA and RFECV:\", list(selected_features_anova))\n",
    "print(\"Features selected with MI and RFECV::\", list(selected_features_mi))\n",
    "\n",
    "print(\"Number of features selected with ANOVA and RFECV:\", sum(rfecv_anova.support_))\n",
    "print(\"Number of features  selected with MI and RFECV:\", sum(rfecv_mi.support_))\n",
    "\n",
    "# plot the number of features vs. accuracy\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv_anova.cv_results_[\"mean_test_score\"]) + 1), rfecv_anova.cv_results_[\"mean_test_score\"], label=\"ANOVA + RFECV\", color=\"blue\")\n",
    "plt.plot(range(1, len(rfecv_mi.cv_results_[\"mean_test_score\"]) + 1), rfecv_mi.cv_results_[\"mean_test_score\"], label=\"MI + RFECV\", color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inge: Optimization based feature selection > Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Encode y labels to numeric values\n",
    "le = LabelEncoder()\n",
    "y_train_numeric = le.fit_transform(y_train)\n",
    "y_test_numeric = le.transform(y_test)\n",
    "\n",
    "# Store the mapping of labels\n",
    "legend_labels = le.classes_  # This saves the original class names\n",
    "print(\"Label Mapping:\", {i: label for i, label in enumerate(legend_labels)})\n",
    "\n",
    "# Define LassoCV with cross-validation\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(-10, -1, n_alphas)# Testing alpha from 0.0001 to 10\n",
    "random_state = 42 #  Using int will produce the same results everytime, 42 is along 0 the most popular choice\n",
    "lasso_cv = LassoCV(cv=5, alphas=alphas, random_state=random_state, max_iter=10000)  # TODO: cv = 5 chosen, what do we want?\n",
    "\n",
    "# Fit LassoCV on training data\n",
    "lasso_cv.fit(X_scaled_standard_train, y_train_numeric)\n",
    "\n",
    "# Get the best alpha value\n",
    "best_alpha = lasso_cv.alpha_\n",
    "print(f\"Best alpha found: {best_alpha}\")\n",
    "\n",
    "# Train final Lasso model with optimal alpha\n",
    "lasso = Lasso(alpha=best_alpha, fit_intercept=False)\n",
    "lasso.fit(X_scaled_standard_train, y_train_numeric)\n",
    "\n",
    "# Select features\n",
    "selector = SelectFromModel(lasso, prefit=True)\n",
    "X_train_selected = selector.transform(X_scaled_standard_train)\n",
    "X_test_selected = selector.transform(X_scaled_standard_test)\n",
    "\n",
    "# Get selected feature indices and names\n",
    "selected_features = np.where(selector.get_support())[0]\n",
    "selected_feature_names = X_scaled_standard_train.columns[selected_features]\n",
    "print(\"Selected Features:\", selected_feature_names)\n",
    "\n",
    "# #############################################################################\n",
    "# #############################################################################\n",
    "\n",
    "print('Lasso like E2.3 which is a classifier')\n",
    "from time import time\n",
    "\n",
    "# Construct classifiers\n",
    "coefs = []\n",
    "accuracies = []\n",
    "times = []\n",
    "for a in alphas:\n",
    "    # Fit classifier\n",
    "    clf = Lasso(alpha=a, fit_intercept=False)\n",
    "    t0 = time()\n",
    "    clf.fit(X_scaled_standard_train, y_train_numeric)\n",
    "    duration = time() - t0\n",
    "    y_pred = clf.predict(X_scaled_standard_train)\n",
    "    message = (\"\\t Misclassified: %d / %d\" % ((y_train_numeric != y_pred).sum(), y_train_numeric.shape[0])) # TODO: GEEN TEST!!\n",
    "\n",
    "    print(message)\n",
    "\n",
    "    # Append statistics\n",
    "    accuracy = float((y_train_numeric != y_pred).sum()) / float(y_train_numeric.shape[0])\n",
    "    times.append(duration)\n",
    "    accuracies.append(accuracy)\n",
    "    coefs.append(clf.coef_)\n",
    "\n",
    "# #############################################################################\n",
    "# Display results\n",
    "\n",
    "# Weights\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, np.squeeze(coefs))\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "# Performance\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, accuracies)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('accuracies')\n",
    "plt.title('Performance as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "# Times\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, times)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('times (s)')\n",
    "plt.title('Fitting time as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "# #############################################################################\n",
    "# #############################################################################\n",
    "\n",
    "print('Feature extraction: PCA')\n",
    "# Apply PCA (only fit on training data)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_features = 4  # TODO: determine what n_features should become\n",
    "pca = PCA(n_components=n_features)\n",
    "X_scaled_standard_train_PCA = pca.fit_transform(X_scaled_standard_train)\n",
    "X_scaled_standard_test_PCA = pca.transform(X_scaled_standard_test)  # Use the same transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing feature selections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "from sklearn import metrics\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Zet de labels om naar numerieke waarden\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)  # Zet 'lipoma' en 'liposarcoma' om naar 0 en 1\n",
    "y_test = le.transform(y_test)  # Pas dezelfde transformatie toe op y_test\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(max_iter=5000),\n",
    "    SGDClassifier(max_iter=5000, tol=1e-3),\n",
    "    KNeighborsClassifier(n_neighbors=40),\n",
    "    DecisionTreeClassifier(random_state=42)\n",
    "]\n",
    "\n",
    "# Store fitted classifiers\n",
    "clfs_fit = []\n",
    "\n",
    "# Select the same features for the test set\n",
    "X_train_selected = X_filtered_anova  # Train-set met ANOVA-geselecteerde features\n",
    "X_test_selected = selector_anova.transform(X_test)  # Test-set met ANOVA-features\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "for clf in classifiers:\n",
    "    # Train classifier\n",
    "    clf.fit(X_scaled_robust_train, y_train)\n",
    "    Y_pred = clf.predict(X_scaled_robust_test)\n",
    "\n",
    "    # Store fitted classifier\n",
    "    clfs_fit.append(clf)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = metrics.accuracy_score(y_test, Y_pred)\n",
    "\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        y_score = clf.predict_proba(X_scaled_robust_test)[:, 1]  # Probability for class 1\n",
    "    else:\n",
    "        y_score = Y_pred  # Use binary predictions if probability is unavailable\n",
    "\n",
    "    auc = metrics.roc_auc_score(y_test, y_score)\n",
    "    f1 = metrics.f1_score(y_test, Y_pred)\n",
    "    precision = metrics.precision_score(y_test, Y_pred)\n",
    "    recall = metrics.recall_score(y_test, Y_pred)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Classifier: {clf.__class__.__name__}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "Random forest, decision tree and bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Define models and parameter distributions\n",
    "models = {\n",
    "    \"Random Forest\": (RandomForestClassifier(), {\n",
    "        'n_estimators': randint(5, 200),\n",
    "        'max_depth': randint(3, 30),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'bootstrap': [True, False]\n",
    "    }),\n",
    "    \"Bagging\": (BaggingClassifier(DecisionTreeClassifier()), {\n",
    "        'n_estimators': randint(5, 200),\n",
    "        'estimator__max_depth': randint(3, 30),\n",
    "        'estimator__min_samples_split': randint(2, 20),\n",
    "        'bootstrap': [True, False]\n",
    "    }),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), {\n",
    "        'max_depth': randint(3, 30),\n",
    "        'min_samples_split': randint(2, 20)\n",
    "    })\n",
    "}\n",
    "\n",
    "# Perform Randomized Search and store results\n",
    "best_estimators = {}\n",
    "best_params = {}\n",
    "best_scores = {}\n",
    "\n",
    "for name, (model, param_dist) in models.items():\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,\n",
    "        scoring='accuracy',\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train_encoded) # Fitted on data without scaling and feature selection\n",
    "    \n",
    "    best_estimators[name] = search.best_estimator_\n",
    "    best_params[name] = search.best_params_\n",
    "    best_scores[name] = search.best_score_\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"\\nBest {model_name}: {best_estimators[model_name]}\")\n",
    "    print(f\"Best {model_name} Parameters: {best_params[model_name]}\")\n",
    "    print(f\"Best {model_name} Accuracy: {best_scores[model_name]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define base SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define parameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],  # Regularization parameter\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10, 100],  # Kernel coefficient\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Different kernel types\n",
    "    'degree': [1, 2, 3, 4, 5, 6],  # Only used for poly kernel\n",
    "}\n",
    "\n",
    "#TIJDELIJK TODO weghalen\n",
    "X_small = X_scaled_robust_train.iloc[:, [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]]  # First 10 features\n",
    "\n",
    "# Randomized Search with cv=10\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=svm_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,  # More iterations for better results\n",
    "    scoring='accuracy',\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on robustly scaled training data\n",
    "random_search.fit(X_small, y_train_encoded) #TODO data nog veranderen\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_svm = random_search.best_estimator_\n",
    "best_params_svm = random_search.best_params_\n",
    "best_score_svm = random_search.best_score_\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Best SVM Model After Randomized Search ===\")\n",
    "print(f\"Best SVM: {best_svm}\")\n",
    "print(f\"Best SVM Parameters: {best_params_svm}\")\n",
    "print(f\"Best SVM Accuracy: {best_score_svm:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
