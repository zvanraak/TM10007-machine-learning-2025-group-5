{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples: 115\n",
      "The number of columns: 494\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "#from worcgist.load_data import load_data\n",
    "from worclipo.load_data import load_data\n",
    "#from worcliver.load_data import load_data\n",
    "#from ecg.load_data import load_data\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn import model_selection\n",
    "\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')\n",
    "\n",
    "X = data.drop(\"label\",axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data\n",
    "* Part 1: Finding missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n",
      "PREDICT_original_sf_area_min_2.5D                                       1\n",
      "PREDICT_original_tf_LBP_min_R3_P12                                     86\n",
      "PREDICT_original_tf_LBP_min_R8_P24                                     73\n",
      "PREDICT_original_tf_LBP_quartile_range_R8_P24                           5\n",
      "PREDICT_original_tf_LBP_min_R15_P36                                    59\n",
      "PREDICT_original_tf_LBP_peak_R15_P36                                    1\n",
      "PREDICT_original_tf_LBP_peak_position_R15_P36                           1\n",
      "PREDICT_original_tf_LBP_quartile_range_R15_P36                         45\n",
      "PREDICT_original_vf_Frangi_full_min_SR(1.0, 10.0)_SS2.0                84\n",
      "PREDICT_original_vf_Frangi_full_median_SR(1.0, 10.0)_SS2.0             82\n",
      "PREDICT_original_vf_Frangi_full_peak_SR(1.0, 10.0)_SS2.0               86\n",
      "PREDICT_original_vf_Frangi_full_peak_position_SR(1.0, 10.0)_SS2.0      86\n",
      "PREDICT_original_vf_Frangi_full_quartile_range_SR(1.0, 10.0)_SS2.0      2\n",
      "PREDICT_original_vf_Frangi_edge_min_SR(1.0, 10.0)_SS2.0                84\n",
      "PREDICT_original_vf_Frangi_edge_median_SR(1.0, 10.0)_SS2.0             82\n",
      "PREDICT_original_vf_Frangi_edge_peak_SR(1.0, 10.0)_SS2.0               86\n",
      "PREDICT_original_vf_Frangi_edge_peak_position_SR(1.0, 10.0)_SS2.0      86\n",
      "PREDICT_original_vf_Frangi_edge_quartile_range_SR(1.0, 10.0)_SS2.0      2\n",
      "PREDICT_original_vf_Frangi_inner_min_SR(1.0, 10.0)_SS2.0               85\n",
      "PREDICT_original_vf_Frangi_inner_median_SR(1.0, 10.0)_SS2.0            80\n",
      "PREDICT_original_vf_Frangi_inner_peak_SR(1.0, 10.0)_SS2.0              86\n",
      "PREDICT_original_vf_Frangi_inner_peak_position_SR(1.0, 10.0)_SS2.0     86\n",
      "PREDICT_original_vf_Frangi_inner_quartile_range_SR(1.0, 10.0)_SS2.0     5\n",
      "PREDICT_original_phasef_monogenic_min_WL3_N5                            2\n",
      "PREDICT_original_phasef_monogenic_peak_WL3_N5                           5\n",
      "PREDICT_original_phasef_monogenic_peak_position_WL3_N5                  5\n",
      "PREDICT_original_phasef_phasecong_min_WL3_N5                           86\n",
      "PREDICT_original_phasef_phasecong_median_WL3_N5                        82\n",
      "PREDICT_original_phasef_phasecong_peak_WL3_N5                          86\n",
      "PREDICT_original_phasef_phasecong_peak_position_WL3_N5                 86\n",
      "PREDICT_original_phasef_phasecong_quartile_range_WL3_N5                 7\n",
      "PREDICT_original_phasef_phasesym_min_WL3_N5                            86\n",
      "PREDICT_original_phasef_phasesym_median_WL3_N5                         62\n",
      "PREDICT_original_phasef_phasesym_peak_WL3_N5                           86\n",
      "PREDICT_original_phasef_phasesym_peak_position_WL3_N5                  86\n",
      "PREDICT_original_phasef_phasesym_quartile_range_WL3_N5                 11\n",
      "dtype: int64\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Define missing value indicators\n",
    "custom_missing = ['NA', 'null', 'N/A', '?', 'None', 'none', '-']\n",
    "\n",
    "# Count NaNs\n",
    "nan_counts = X_train.isna().sum()\n",
    "nan_counts_selected = nan_counts[nan_counts !=0]\n",
    "\n",
    "# Count empty strings\n",
    "empty_string_counts = (X_train == '').sum()\n",
    "empty_string_counts_selected = empty_string_counts[empty_string_counts != 0]\n",
    "\n",
    "# Count zero's\n",
    "zero_counts = (X_train == 0).sum()\n",
    "zero_counts_selected = zero_counts[zero_counts != 0]\n",
    "\n",
    "# Count custom missing indicators (case-insensitive match)\n",
    "custom_missing_counts = X_train.apply(lambda col: col.astype(str).str.lower().isin([val.lower() for val in custom_missing]).sum())\n",
    "custom_missing_counts_selected = custom_missing_counts[custom_missing_counts !=0]\n",
    "\n",
    "# Printing\n",
    "print(nan_counts_selected)\n",
    "print(empty_string_counts_selected)\n",
    "print(zero_counts_selected)\n",
    "print(custom_missing_counts_selected)\n",
    "#X_train_zero_columns = X_train[zero_counts_selected.index]\n",
    "#X_train_zero_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Part 2: Processing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data was only find in the form of zero's, therefore only zero_counts is further used.\n",
    "limit = len(X_train.index)*50/100 #If 50% or more of the data within one feature is missing the feature is deleted\n",
    "valid_columns = [col for col, count in zero_counts.items() if count < limit]\n",
    "\n",
    "# Keep only the valid columns in both X_train and X_test\n",
    "X_train = X_train[valid_columns]\n",
    "X_test = X_test[valid_columns]\n",
    "\n",
    "# Imputate remaining zero's\n",
    "# Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Replace zeros with NaN\n",
    "X_train.replace(0, np.nan, inplace=True)\n",
    "X_test.replace(0, np.nan, inplace=True)\n",
    "\n",
    "imputation_strategies = {}  # Store mean/median decision per column\n",
    "\n",
    "for col in X_train.select_dtypes(include=['number']).columns:  # Only numeric columns\n",
    "    col_data = X_train[col].dropna()  # Remove NaN values for testing\n",
    "\n",
    "    if len(col_data) > 3:  # Shapiro requires at least 3 non-null values\n",
    "        if col_data.nunique() == 1:  # Check if all values are the same\n",
    "            strategy = 'median'  # Default to median if no variability\n",
    "        else:\n",
    "            _, p = shapiro(col_data)\n",
    "            strategy = 'mean' if p > 0.05 else 'median'\n",
    "    else:\n",
    "        strategy = 'median'  # Default to median if too few values\n",
    "\n",
    "    imputation_strategies[col] = strategy\n",
    "\n",
    "# Create imputers for mean and median\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "for col, strategy in imputation_strategies.items():\n",
    "    imputer = mean_imputer if strategy == 'mean' else median_imputer\n",
    "    X_train[[col]] = imputer.fit_transform(X_train[[col]])\n",
    "    X_test[[col]] = imputer.transform(X_test[[col]])  # Use the same imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler_MaxAbsScaler = preprocessing.MaxAbsScaler()\n",
    "scaler_Normalizer = preprocessing.Normalizer()\n",
    "scaler_standard = preprocessing.StandardScaler()\n",
    "scaler_minmax = preprocessing.MinMaxScaler()\n",
    "scaler_robust = preprocessing.RobustScaler()\n",
    "\n",
    "scaled_MaxAbsScaler_array_train = scaler_MaxAbsScaler.fit_transform(X_train)\n",
    "scaled_MaxAbsScaler_array_test = scaler_MaxAbsScaler.transform(X_test)\n",
    "scaled_Normalizer_array_train = scaler_Normalizer.fit_transform(X_train)\n",
    "scaled_Normalizer_array_test = scaler_Normalizer.transform(X_test)\n",
    "scaled_standard_array_train = scaler_standard.fit_transform(X_train)\n",
    "scaled_standard_array_test = scaler_standard.transform(X_test)\n",
    "scaled_minmax_array_train = scaler_minmax.fit_transform(X_train)\n",
    "scaled_minmax_array_test = scaler_minmax.transform(X_test)\n",
    "scaled_robust_array_train = scaler_robust.fit_transform(X_train)\n",
    "scaled_robust_array_test = scaler_robust.transform(X_test)\n",
    "\n",
    "X_scaled_MaxAbsScaler_train = pd.DataFrame(scaled_MaxAbsScaler_array_train, columns=X_train.columns)\n",
    "X_df_scaled_Normalizer_train = pd.DataFrame(scaled_Normalizer_array_train, columns=X_train.columns)\n",
    "X_scaled_Normalizer_test = pd.DataFrame(scaled_Normalizer_array_test, columns=X_test.columns)\n",
    "X_scaled_standard_train = pd.DataFrame(scaled_standard_array_train, columns=X_train.columns)\n",
    "X_scaled_standard_test = pd.DataFrame(scaled_standard_array_test, columns=X_test.columns)\n",
    "X_scaled_minmax_train = pd.DataFrame(scaled_minmax_array_train, columns=X_train.columns)\n",
    "X_scaled_minmax_test = pd.DataFrame(scaled_minmax_array_test, columns=X_test.columns)\n",
    "X_scaled_robust_train = pd.DataFrame(scaled_robust_array_train, columns=X_train.columns)\n",
    "X_scaled_robust_test = pd.DataFrame(scaled_robust_array_test, columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lloyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorplot(clf, ax, x, y, h=100):\n",
    "    '''\n",
    "    Overlay the decision areas as colors in an axes.\n",
    "\n",
    "    Input:\n",
    "        clf: trained classifier\n",
    "        ax: axis to overlay color mesh on\n",
    "        x: feature on x-axis\n",
    "        y: feature on y-axis\n",
    "        h(optional): steps in the mesh\n",
    "    '''\n",
    "    # Create a meshgrid the size of the axis\n",
    "    xstep = (x.max() - x.min() ) / 20.0\n",
    "    ystep = (y.max() - y.min() ) / 20.0\n",
    "    x_min, x_max = x.min() - xstep, x.max() + xstep\n",
    "    y_min, y_max = y.min() - ystep, y.max() + ystep\n",
    "    h = max((x_max - x_min, y_max - y_min))/h\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    if len(Z.shape) > 1:\n",
    "        Z = Z[:, 1]\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    cm = plt.cm.RdBu_r\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "    del xx, yy, x_min, x_max, y_min, y_max, Z, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Mean Accuracy (5-fold CV): 0.72\n",
      "Random Forest - Mean Accuracy (5-fold CV): 0.73\n",
      "Homemade Random Forest - Mean Accuracy (5-fold CV): 0.76\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Define classifiers\n",
    "homemade_random_forest = BaggingClassifier(DecisionTreeClassifier())\n",
    "\n",
    "clsfs = [\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"Homemade Random Forest\", homemade_random_forest)\n",
    "]\n",
    "\n",
    "# Encode categorical labels if needed\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Convert to numeric labels\n",
    "\n",
    "# Evaluate classifiers with cross-validation\n",
    "for name, clf in clsfs:\n",
    "    # Perform 5-fold cross-validation\n",
    "    scores = cross_val_score(clf, X_train, y_train_encoded, cv=5)\n",
    "    \n",
    "    # Compute mean accuracy\n",
    "    mean_acc = np.mean(scores)\n",
    "    \n",
    "    # Print the accuracy\n",
    "    print(f\"{name} - Mean Accuracy (5-fold CV): {mean_acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
