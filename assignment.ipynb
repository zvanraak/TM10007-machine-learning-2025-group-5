{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "#from worcgist.load_data import load_data\n",
    "from worclipo.load_data import load_data\n",
    "#from worcliver.load_data import load_data\n",
    "#from ecg.load_data import load_data\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')\n",
    "\n",
    "X = data.drop(\"label\",axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    X_train_fold = X_train.iloc[train_idx]\n",
    "    y_train_fold = y_train.iloc[train_idx]\n",
    "    X_val_fold = X_train.iloc[val_idx]\n",
    "    y_val_fold = y_train.iloc[val_idx]\n",
    "\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\" - Train size: {len(X_train_fold)}\")\n",
    "    print(f\" - Validation size: {len(X_val_fold)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data\n",
    "* Part 1: Finding missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define missing value indicators\n",
    "custom_missing = ['NA', 'N/A', '?', 'None', 'none', '-']\n",
    "\n",
    "# Count NaNs\n",
    "nan_counts = X_train.isna().sum()\n",
    "\n",
    "# Count empty strings\n",
    "empty_string_counts = (X_train == '').sum()\n",
    "\n",
    "# Count custom missing indicators (case-insensitive match)\n",
    "custom_missing_counts = X_train.apply(lambda col: col.astype(str).str.lower().isin([val.lower() for val in custom_missing]).sum())\n",
    "\n",
    "# Compute total missing count per column\n",
    "total_missing = nan_counts + empty_string_counts + custom_missing_counts\n",
    "\n",
    "# Filter out columns where total missing is zero\n",
    "total_missing_selected = total_missing[total_missing != 0]\n",
    "\n",
    "# Print total missing counts\n",
    "print(total_missing_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Part 2: Processing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Replacing missing values with NaN\n",
    "custom_missing = ['NA', 'N/A', '?', 'None', 'none', '-', '']\n",
    "X_train.replace(custom_missing, np.nan, inplace=True)\n",
    "X_test.replace(custom_missing, np.nan, inplace=True)\n",
    "\n",
    "# If 50% or more of the data within one feature is missing the feature is deleted\n",
    "limit = len(X_train.index)*50/100\n",
    "valid_columns = [col for col, count in total_missing.items() if count < limit]\n",
    "\n",
    "# Keep only the valid columns in both X_train and X_test\n",
    "X_train = X_train[valid_columns]\n",
    "X_test = X_test[valid_columns]\n",
    "\n",
    "# Imputate \n",
    "\n",
    "# Check if imputation is needed\n",
    "if X_train.isna().sum().sum() == 0:\n",
    "    pass\n",
    "else:\n",
    "    # Dictionary to store mean/median decision per column\n",
    "    imputation_strategies = {}\n",
    "\n",
    "    for col in X_train.select_dtypes(include=['number']).columns:  # Only numeric columns\n",
    "        col_data = X_train[col].dropna()  # Remove NaN values for testing\n",
    "\n",
    "        if len(col_data) > 3:  # Shapiro requires at least 3 non-null values\n",
    "            if col_data.nunique() == 1:  # Check if all values are the same\n",
    "                strategy = 'median'  # Default to median if no variability\n",
    "            else:\n",
    "                _, p = shapiro(col_data)\n",
    "                strategy = 'mean' if p > 0.05 else 'median'\n",
    "        else:\n",
    "            strategy = 'median'  # Default to median if too few values\n",
    "\n",
    "        imputation_strategies[col] = strategy\n",
    "\n",
    "    # Create imputers for mean and median\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Apply imputers for each feature\n",
    "    for col, strategy in imputation_strategies.items():\n",
    "        imputer = mean_imputer if strategy == 'mean' else median_imputer\n",
    "        X_train[col] = imputer.fit_transform(X_train[[col]])\n",
    "        X_test[col] = imputer.transform(X_test[[col]])  # Use the same imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler_robust = preprocessing.RobustScaler()\n",
    "\n",
    "scaled_robust_array_train = scaler_robust.fit_transform(X_train)\n",
    "scaled_robust_array_test = scaler_robust.transform(X_test)\n",
    "\n",
    "X_scaled_robust_train = pd.DataFrame(scaled_robust_array_train, columns=X_train.columns)\n",
    "X_scaled_robust_test = pd.DataFrame(scaled_robust_array_test, columns=X_test.columns)\n",
    "\n",
    "# print(X_scaled_robust_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lloyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "Random forest, decision tree and bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Define models and parameter distributions\n",
    "models = {\n",
    "    \"Random Forest\": (RandomForestClassifier(), {\n",
    "        'n_estimators': randint(5, 200),\n",
    "        'max_depth': randint(3, 30),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'bootstrap': [True, False]\n",
    "    }),\n",
    "    \"Bagging\": (BaggingClassifier(DecisionTreeClassifier()), {\n",
    "        'n_estimators': randint(5, 200),\n",
    "        'estimator__max_depth': randint(3, 30),\n",
    "        'estimator__min_samples_split': randint(2, 20),\n",
    "        'bootstrap': [True, False]\n",
    "    }),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), {\n",
    "        'max_depth': randint(3, 30),\n",
    "        'min_samples_split': randint(2, 20)\n",
    "    })\n",
    "}\n",
    "\n",
    "# Perform Randomized Search and store results\n",
    "best_estimators = {}\n",
    "best_params = {}\n",
    "best_scores = {}\n",
    "\n",
    "for name, (model, param_dist) in models.items():\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,\n",
    "        scoring='accuracy',\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train_encoded) # Fitted on data without scaling and feature selection\n",
    "    \n",
    "    best_estimators[name] = search.best_estimator_\n",
    "    best_params[name] = search.best_params_\n",
    "    best_scores[name] = search.best_score_\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"\\nBest {model_name}: {best_estimators[model_name]}\")\n",
    "    print(f\"Best {model_name} Parameters: {best_params[model_name]}\")\n",
    "    print(f\"Best {model_name} Accuracy: {best_scores[model_name]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define base SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define parameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],  # Regularization parameter\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10, 100],  # Kernel coefficient\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Different kernel types\n",
    "    'degree': [1, 2, 3, 4, 5, 6],  # Only used for poly kernel\n",
    "}\n",
    "\n",
    "#TIJDELIJK TODO weghalen\n",
    "X_small = X_scaled_robust_train.iloc[:, [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]]  # First 10 features\n",
    "\n",
    "# Randomized Search with cv=10\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=svm_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,  # More iterations for better results\n",
    "    scoring='accuracy',\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on robustly scaled training data\n",
    "random_search.fit(X_small, y_train_encoded) #TODO data nog veranderen\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_svm = random_search.best_estimator_\n",
    "best_params_svm = random_search.best_params_\n",
    "best_score_svm = random_search.best_score_\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Best SVM Model After Randomized Search ===\")\n",
    "print(f\"Best SVM: {best_svm}\")\n",
    "print(f\"Best SVM Parameters: {best_params_svm}\")\n",
    "print(f\"Best SVM Accuracy: {best_score_svm:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
